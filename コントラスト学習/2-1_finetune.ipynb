{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries ğŸ“š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import easydict\n",
    "from glob import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch.utils.data as data\n",
    "from tqdm import tqdm\n",
    "from natsort import natsorted\n",
    "from glob import glob\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "import warnings\n",
    "import tensorboardX as tbx\n",
    "import csv\n",
    "\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random seedğŸŒ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#å†ç¾æ€§ã®ç¢ºä¿\n",
    "seed = 44\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    #Disabling the benchmark by CUDA convolution operation(GPUã‚’ä½¿ã†ã¨ãã®å†ç¾æ€§ã®æ‹…ä¿) (https://pytorch.org/docs/stable/notes/randomness.html)\n",
    "    torch.backends.cudnn.benchmark = False \n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments ğŸ“—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict({\n",
    "    \"batch_size\":32, # åˆ†å‰²æ•°ï¼batch_size/2\n",
    "    \"epochs\":100000,\n",
    "    \"learning_rate\" : 0.00001, # æå¤±ãŒä¸‹ãŒã‚Šãã‚‰ãªã„å ´åˆã¯ä¸‹ã’ã‚‹ã¨ã„ã„ã‹ã‚‚ï¼Ÿã€€adamã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ0.001\n",
    "    \"early_stop_patience\" : 15,\n",
    "    \n",
    "    \"model_save_path\" : \"Checkpoints\",\n",
    "    \"load_model_file_name\" : \"model_data40000\",\n",
    "    \"save_model_file_name\" : \"model_data40000_finetuned\",\n",
    "\n",
    "    \"train_size\" : 40000,\n",
    "    \"valid_size\" : 10000\n",
    "    \n",
    "    #\"gamma\" : 1, #å›å¸°æå¤±ã‚’é‡è¦–ã™ã‚‹å‰²åˆ\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU ğŸ“º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_GPU():\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒã‚¤ã‚¹ã‚’å‡ºåŠ›ã™ã‚‹é–¢æ•°\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    device : object\n",
    "        GPUãŒä½¿ãˆã‚‹ãªã‚‰'cuda:0',ä½¿ãˆãªã„ãªã‚‰'cpu'ã‚’è¿”ã™\n",
    "    \"\"\"\n",
    "    print(\"Check GPU\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"You can use GPU({torch.cuda.get_device_name()})\")\n",
    "        d_type = \"cuda:0\"\n",
    "    else:\n",
    "        print(\"You use cpu\")\n",
    "        d_type = \"cpu\"\n",
    "    print(\"-----\")\n",
    "    device = torch.device(d_type)\n",
    "    return device "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image & Label ğŸ“‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_divisible(a:int, b:int):\n",
    "    \"\"\"\n",
    "    ãƒãƒƒãƒã‚µã‚¤ã‚ºã§å‰²ã‚Šåˆ‡ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿æ•°ã‚’æ±‚ã‚ã‚‹ãŸã‚ã®é–¢æ•°\n",
    "    \"\"\"\n",
    "    remainder = a % b\n",
    "    if remainder == 0:\n",
    "        return int(a)  # aã¯ã™ã§ã«bã§å‰²ã‚Šåˆ‡ã‚Œã‚‹\n",
    "    elif remainder >= b / 2:\n",
    "        return int(a + (b - remainder))  # aã‚’å¢—ã‚„ã—ã¦bã®æ¬¡ã®å€æ•°ã«ã™ã‚‹\n",
    "    else:\n",
    "        return int(a - remainder)  # aã‚’æ¸›ã‚‰ã—ã¦bã®ç¾åœ¨ã®å€æ•°ã«ã™ã‚‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Normalization ğŸ“Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_normalization():\n",
    "    \"\"\"\n",
    "    å¹³å‡å€¤ï¼Œæ¨™æº–åå·®ã‚’æ±‚ã‚ã‚‹\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    mean : tuple\n",
    "        ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç”»ç´ ã®å¹³å‡\n",
    "    std : tuple\n",
    "        ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç”»ç´ ã®æ¨™æº–åå·®\n",
    "    \"\"\"\n",
    "    df_ms = pd.read_csv(f\"mean_std.csv\")\n",
    "    mean = df_ms[\"mean\"][0]\n",
    "    std = df_ms[\"std\"][0]\n",
    "\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data set ğŸ§°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTransform:\n",
    "    \"\"\"\n",
    "    ç”»åƒã®å‰å‡¦ç†ã‚¯ãƒ©ã‚¹\n",
    "    è¨“ç·´æ™‚ã ã‘ã€ãƒ‡ãƒ¼ã‚¿ã‚ªãƒ¼ã‚®ãƒ¥ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³(DA)ãŒã§ãã‚‹ã‚ˆã†ã«ã€train,validã§åˆ†ã‘ã¦æ›¸ã„ãŸ\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    mean : int\n",
    "        ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç”»ç´ å€¤ã®å¹³å‡å€¤\n",
    "    std : int\n",
    "        ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç”»ç´ å€¤ã®æ¨™æº–åå·®\n",
    "    \"\"\"   \n",
    "\n",
    "    def __init__(self, mean, std):\n",
    "        self.data_transform = {\n",
    "            'train':transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std) \n",
    "            ]),\n",
    "            'valid':transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std)\n",
    "            ])\n",
    "        }\n",
    "    \n",
    "    #å¼•æ•°ãªã—ã§å‘¼ã°ã‚ŒãŸã¨ãã®æŒ™å‹•ã‚’å®šç¾©\n",
    "    def __call__(self, img, phase='train'):\n",
    "        return self.data_transform[phase](img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image_Dataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    ç”»åƒã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¯ãƒ©ã‚¹ã€‚\n",
    "    Pytorch Dataset class ã‚’ç¶™æ‰¿\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    file_list : list\n",
    "        ç”»åƒã®path_list\n",
    "    label_list : list\n",
    "        ãƒ©ãƒ™ãƒ«ã®path_list       \n",
    "    transform : object\n",
    "        class ImageTransform()    \n",
    "    phase : str\n",
    "        'train' or 'valid'\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df, transform, phase='train'):\n",
    "        self.file_list = df[\"fname\"] \n",
    "        self.labels = df[[\"dx\",\"dy\"]].to_numpy()\n",
    "        #self.l = df[\"l\"]\n",
    "        self.d = df[\"d\"]\n",
    "        self.transform = transform  \n",
    "        self.phase = phase  \n",
    "\n",
    "    #ã“ã®ã‚¯ãƒ©ã‚¹ã®ç”»åƒæšæ•°ã‚’å®šç¾©ã€‚\n",
    "    def __len__(self):\n",
    "        return len(self.file_list) \n",
    "\n",
    "    #ã“ã®ã‚¯ãƒ©ã‚¹ã«è§’æ‹¬å¼§ã§ã‚¢ã‚¯ã‚»ã‚¹ã—ãŸæ™‚ã®æŒ™å‹•ã‚’å®šç¾©\n",
    "    def __getitem__(self, index):\n",
    "        # ç”»åƒã‚’ãƒ­ãƒ¼ãƒ‰\n",
    "        img_path = self.file_list[index]\n",
    "        img = Image.open(img_path)\n",
    "        #img = img.convert(\"L\") #ã‚°ãƒ¬ã‚¤ã‚¹ã‚±ãƒ¼ãƒ« \n",
    "        # å‰å‡¦ç†\n",
    "        img_transformed = self.transform(\n",
    "            img, self.phase)  # torch.Size([width, height]) \n",
    "\n",
    "        #label\n",
    "        label = self.labels[index]\n",
    "        #Tensorã«å¤‰æ›\n",
    "        label = torch.tensor(label, dtype=torch.float32) \n",
    "\n",
    "        # ç©ºæ°—æºã‚‰ãé‡\n",
    "        d = self.d[index]           \n",
    "\n",
    "        return img_transformed, label, d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NetWork ğŸ§ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ãƒ¢ãƒ‡ãƒ«\n",
    "\n",
    "# # æ®‹å·®ãƒ–ãƒ­ãƒƒã‚¯\n",
    "class block(nn.Module):\n",
    "    def __init__(self, first_conv_in_channels, first_conv_out_channels, identity_conv=None, stride=1):\n",
    "        \"\"\"\n",
    "        æ®‹å·®ãƒ–ãƒ­ãƒƒã‚¯ã‚’ä½œæˆã™ã‚‹ã‚¯ãƒ©ã‚¹\n",
    "        Args:\n",
    "            first_conv_in_channels : 1ç•ªç›®ã®convå±¤ï¼ˆ1Ã—1ï¼‰ã®input channelæ•°\n",
    "            first_conv_out_channels : 1ç•ªç›®ã®convå±¤ï¼ˆ1Ã—1ï¼‰ã®output channelæ•°\n",
    "            identity_conv : channelæ•°èª¿æ•´ç”¨ã®convå±¤\n",
    "            stride : 3Ã—3convå±¤ã«ãŠã‘ã‚‹stideæ•°ã€‚sizeã‚’åŠåˆ†ã«ã—ãŸã„ã¨ãã¯2ã«è¨­å®š\n",
    "        \"\"\"        \n",
    "        super(block, self).__init__()\n",
    "\n",
    "        # 1ç•ªç›®ã®convå±¤ï¼ˆ1Ã—1ï¼‰\n",
    "        self.conv1 = nn.Conv2d(first_conv_in_channels, first_conv_out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(first_conv_out_channels)\n",
    "\n",
    "        # 2ç•ªç›®ã®convå±¤ï¼ˆ3Ã—3ï¼‰\n",
    "        # ãƒ‘ã‚¿ãƒ¼ãƒ³3ã®æ™‚ã¯sizeã‚’å¤‰æ›´ã§ãã‚‹ã‚ˆã†ã«strideã¯å¯å¤‰\n",
    "        self.conv2 = nn.Conv2d(first_conv_out_channels, first_conv_out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(first_conv_out_channels)\n",
    "\n",
    "        # 3ç•ªç›®ã®convå±¤ï¼ˆ1Ã—1ï¼‰\n",
    "        # output channelã¯input channelã®4å€ã«ãªã‚‹\n",
    "        self.conv3 = nn.Conv2d(first_conv_out_channels, first_conv_out_channels*4, kernel_size=1, stride=1, padding=0)\n",
    "        self.bn3 = nn.BatchNorm2d(first_conv_out_channels*4)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # identityã®channelæ•°ã®èª¿æ•´ãŒå¿…è¦ãªå ´åˆã¯convå±¤ï¼ˆ1Ã—1ï¼‰ã‚’ç”¨æ„ã€ä¸è¦ãªå ´åˆã¯None\n",
    "        self.identity_conv = identity_conv\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        identity = x.clone()  # å…¥åŠ›ã‚’ä¿æŒã™ã‚‹\n",
    "\n",
    "        x = self.conv1(x)  # 1Ã—1ã®ç•³ã¿è¾¼ã¿\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv2(x)  # 3Ã—3ã®ç•³ã¿è¾¼ã¿ï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³3ã®æ™‚ã¯strideãŒ2ã«ãªã‚‹ãŸã‚ã€ã“ã“ã§sizeãŒåŠåˆ†ã«ãªã‚‹ï¼‰\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.conv3(x)  # 1Ã—1ã®ç•³ã¿è¾¼ã¿\n",
    "        x = self.bn3(x)\n",
    "\n",
    "        # å¿…è¦ãªå ´åˆã¯convå±¤ï¼ˆ1Ã—1ï¼‰ã‚’é€šã—ã¦identityã®channelæ•°ã®èª¿æ•´ã—ã¦ã‹ã‚‰è¶³ã™\n",
    "        if self.identity_conv is not None:\n",
    "            identity = self.identity_conv(identity)\n",
    "        x += identity\n",
    "\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "#  Resnet50\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self,block):\n",
    "        super(ResNet,self).__init__()\n",
    "\n",
    "        # conv1ã¯ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£é€šã‚Šã«ãƒ™ã‚¿æ‰“ã¡\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        # self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1) # in:(64,112*112)ã€out:(64,56*56)\n",
    "        self.maxpool = nn.Identity()\n",
    "\n",
    "        # conv2_xã¯ã‚µã‚¤ã‚ºã®å¤‰æ›´ã¯ä¸è¦ã®ãŸã‚ã€strideã¯1\n",
    "        self.conv2_x = self._make_layer(block, 3, res_block_in_channels=64, first_conv_out_channels=64, stride=1)\n",
    "\n",
    "        # conv3_xä»¥é™ã¯ã‚µã‚¤ã‚ºã®å¤‰æ›´ã‚’ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ãŸã‚ã€strideã¯2\n",
    "        self.conv3_x = self._make_layer(block, 4, res_block_in_channels=256,  first_conv_out_channels=128, stride=2)\n",
    "        self.conv4_x = self._make_layer(block, 6, res_block_in_channels=512,  first_conv_out_channels=256, stride=2)\n",
    "        self.conv5_x = self._make_layer(block, 3, res_block_in_channels=1024, first_conv_out_channels=512, stride=2)\n",
    "\n",
    "        #self.avgpool = nn.AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        # self.fc1 = nn.Linear(512*4,2)\n",
    "        self.fc1 = nn.Identity()\n",
    "    \n",
    "        # mlp projection head\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(in_features=2048, out_features=2048),\n",
    "            nn.BatchNorm1d(2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=2048, out_features=128),\n",
    "            nn.BatchNorm1d(128),\n",
    "        )\n",
    "        # Regression\n",
    "        self.regression = nn.Sequential(\n",
    "            nn.Linear(2048, 512), \n",
    "            nn.LeakyReLU(),       \n",
    "            nn.Linear(512, 128),  \n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, 32),        \n",
    "            nn.Linear(32, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        x = self.conv1(x)  \n",
    "        x = self.bn1(x)     \n",
    "        x = self.relu(x)    \n",
    "        x = self.maxpool(x) #out:(64*16*16)\n",
    "        \n",
    "\n",
    "        x = self.conv2_x(x)  # in:(64,56*56)  ã€out:(256,56*56)\n",
    "        x = self.conv3_x(x)  # in:(256,56*56) ã€out:(512,28*28)\n",
    "        x = self.conv4_x(x)  # in:(512,28*28) ã€out:(1024,14*14)\n",
    "        x = self.conv5_x(x)  # in:(1024,14*14)ã€out:(2048,7*7)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.reshape(x.shape[0], -1) # 2048\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        #projection_output = self.projection(backbone_output)\n",
    "        regression_output = self.regression(x)\n",
    "\n",
    "        return regression_output\n",
    "\n",
    "    def _make_layer(self, block, num_res_blocks, res_block_in_channels, first_conv_out_channels, stride):\n",
    "        layers = []\n",
    "\n",
    "        # 1ã¤ç›®ã®æ®‹å·®ãƒ–ãƒ­ãƒƒã‚¯ã§ã¯channelèª¿æ•´ã€åŠã³sizeèª¿æ•´ãŒç™ºç”Ÿã™ã‚‹\n",
    "        # identifyã‚’è¶³ã™å‰ã«1Ã—1ã®convå±¤ã‚’è¿½åŠ ã—ã€ã‚µã‚¤ã‚ºèª¿æ•´ãŒå¿…è¦ãªå ´åˆã¯strideã‚’2ã«è¨­å®š\n",
    "        identity_conv = nn.Conv2d(res_block_in_channels, first_conv_out_channels*4, kernel_size=1,stride=stride)\n",
    "        layers.append(block(res_block_in_channels, first_conv_out_channels, identity_conv, stride))\n",
    "\n",
    "        # 2ã¤ç›®ä»¥é™ã®input_channelæ•°ã¯1ã¤ç›®ã®output_channelã®4å€\n",
    "        in_channels = first_conv_out_channels*4\n",
    "\n",
    "        # channelèª¿æ•´ã€sizeèª¿æ•´ã¯ç™ºç”Ÿã—ãªã„ãŸã‚ã€identity_convã¯Noneã€strideã¯1\n",
    "        for i in range(num_res_blocks - 1):\n",
    "            layers.append(block(in_channels, first_conv_out_channels, identity_conv=None, stride=1))\n",
    "\n",
    "        return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train ğŸ‹ï¸â€â™‚ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloaders_dict, device, opt, criterion, writer, earlystopping): #ãã‚ãƒ¼ãã™\n",
    "\n",
    "    #global epoch\n",
    "\n",
    "    BREAK = False\n",
    "    print(f\"\\nStarting training\")\n",
    "    for epoch in range(0, args.epochs+1):\n",
    "\n",
    "        # epochã”ã¨ã®å­¦ç¿’ã¨æ¤œè¨¼ã®ãƒ«ãƒ¼ãƒ—\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ãƒ¢ãƒ¼ãƒ‰ã«\n",
    "            else:\n",
    "                model.eval()   # ãƒ¢ãƒ‡ãƒ«ã‚’æ¤œè¨¼ãƒ¢ãƒ¼ãƒ‰ã«\n",
    "\n",
    "            epoch_loss = 0.0  # epochã®æå¤±å’Œ              \n",
    "\n",
    "            #æœªè¨“ç·´æ™‚ã®æ€§èƒ½è©•ä¾¡\n",
    "            if (epoch == 0 and phase=='train'):\n",
    "                continue\n",
    "            \n",
    "            # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‹ã‚‰ãƒŸãƒ‹ãƒãƒƒãƒã‚’å–ã‚Šå‡ºã™ãƒ«ãƒ¼ãƒ—ã€‚args['batch_size']æšã”ã¨å–ã‚Šå‡ºã™\n",
    "            for inputs, labels, _ in tqdm(dataloaders_dict[phase], leave=False, desc='Epoch {}/{} {}'.format(epoch, args.epochs, phase)): \n",
    "\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # optimizerã‚’åˆæœŸåŒ–\n",
    "                opt.zero_grad()\n",
    "\n",
    "                # é †ä¼æ¬ï¼ˆforwardï¼‰è¨ˆç®—\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)  # æå¤±ã‚’è¨ˆç®—ã€‚\n",
    "\n",
    "                    # è¨“ç·´æ™‚ã¯ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ‘ã‚²ãƒ¼ã‚·ãƒ§ãƒ³\n",
    "                    if phase == 'train':\n",
    "                        loss.backward() \n",
    "                        opt.step()\n",
    "\n",
    "                # æå¤±ã®è¨ˆç®—\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            #tensorboardã«å‡ºåŠ›\n",
    "            if phase == 'train':\n",
    "                writer.add_scalar('train_epoch_loss_finetune', epoch_loss, epoch) #(ã‚°ãƒ©ãƒ•å, yåº§æ¨™, xåº§æ¨™)           \n",
    "            elif phase == 'valid':\n",
    "                writer.add_scalar('valid_epoch_loss_finetune', epoch_loss, epoch)                 \n",
    "                #æ¯ã‚¨ãƒãƒƒã‚¯earlystoppingã®åˆ¤å®šã‚’ã•ã›ã‚‹\n",
    "                earlystopping(epoch_loss, model) #callãƒ¡ã‚½ãƒƒãƒ‰å‘¼ã³å‡ºã—\n",
    "                if earlystopping.early_stop: #ã‚¹ãƒˆãƒƒãƒ—ãƒ•ãƒ©ã‚°ãŒTrueã®å ´åˆã€breakã§forãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹\n",
    "                    print(\"Early Stopping!\")\n",
    "                    BREAK = True\n",
    "        if BREAK:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EarlyStopping â›”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    earlystoppingã‚¯ãƒ©ã‚¹\n",
    "    æå¤±ãŒä¸‹ãŒã£ãŸã“ã¨ã‚’åˆ¤æ–­ã—ã¦å­¦ç¿’ã‚’æ‰“ã¡åˆ‡ã‚‹\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    patience : int\n",
    "        ä½•å›æå¤±ãŒä¸‹ãŒã‚‰ãªã‹ã£ãŸã‚‰å­¦ç¿’ã‚’æ‰“ã¡åˆ‡ã‚‹ã‹\n",
    "    verbose : bool\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, patience, verbose):\n",
    "        \"\"\"å¼•æ•° : æœ€å°å€¤ã®éæ›´æ–°æ•°ã‚«ã‚¦ãƒ³ã‚¿ã€è¡¨ç¤ºè¨­å®š\"\"\"\n",
    "\n",
    "        self.patience = patience    #è¨­å®šã‚¹ãƒˆãƒƒãƒ—ã‚«ã‚¦ãƒ³ã‚¿\n",
    "        self.verbose = verbose      #è¡¨ç¤ºã®æœ‰ç„¡\n",
    "        self.counter = 0            #ç¾åœ¨ã®ã‚«ã‚¦ãƒ³ã‚¿å€¤\n",
    "        self.best_score = None      #ãƒ™ã‚¹ãƒˆã‚¹ã‚³ã‚¢\n",
    "        self.early_stop = False     #ã‚¹ãƒˆãƒƒãƒ—ãƒ•ãƒ©ã‚°\n",
    "        self.val_loss_min = np.Inf   #å‰å›ã®ãƒ™ã‚¹ãƒˆã‚¹ã‚³ã‚¢è¨˜æ†¶ç”¨\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        \"\"\"\n",
    "        ç‰¹æ®Š(call)ãƒ¡ã‚½ãƒƒãƒ‰\n",
    "        å®Ÿéš›ã«å­¦ç¿’ãƒ«ãƒ¼ãƒ—å†…ã§æœ€å°lossã‚’æ›´æ–°ã—ãŸã‹å¦ã‹ã‚’è¨ˆç®—ã•ã›ã‚‹éƒ¨åˆ†\n",
    "        \"\"\"\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:  #1Epochç›®ã®å‡¦ç†\n",
    "            self.best_score = score   #1Epochç›®ã¯ãã®ã¾ã¾ãƒ™ã‚¹ãƒˆã‚¹ã‚³ã‚¢ã¨ã—ã¦è¨˜éŒ²ã™ã‚‹\n",
    "            self.checkpoint(val_loss, model)  #è¨˜éŒ²å¾Œã«ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¦ã‚¹ã‚³ã‚¢è¡¨ç¤ºã™ã‚‹\n",
    "        elif score < self.best_score:  # ãƒ™ã‚¹ãƒˆã‚¹ã‚³ã‚¢ã‚’æ›´æ–°ã§ããªã‹ã£ãŸå ´åˆ\n",
    "            self.counter += 1   #ã‚¹ãƒˆãƒƒãƒ—ã‚«ã‚¦ãƒ³ã‚¿ã‚’+1\n",
    "            # if self.verbose:  #è¡¨ç¤ºã‚’æœ‰åŠ¹ã«ã—ãŸå ´åˆã¯çµŒéã‚’è¡¨ç¤º\n",
    "            #     print(f'EarlyStopping counter: {self.counter} out of {self.patience}')  #ç¾åœ¨ã®ã‚«ã‚¦ãƒ³ã‚¿ã‚’è¡¨ç¤ºã™ã‚‹ \n",
    "            if self.counter >= self.patience:  #è¨­å®šã‚«ã‚¦ãƒ³ãƒˆã‚’ä¸Šå›ã£ãŸã‚‰ã‚¹ãƒˆãƒƒãƒ—ãƒ•ãƒ©ã‚°ã‚’Trueã«å¤‰æ›´\n",
    "                self.early_stop = True\n",
    "        else:  #ãƒ™ã‚¹ãƒˆã‚¹ã‚³ã‚¢ã‚’æ›´æ–°ã—ãŸå ´åˆ\n",
    "            self.best_score = score  #ãƒ™ã‚¹ãƒˆã‚¹ã‚³ã‚¢ã‚’ä¸Šæ›¸ã\n",
    "            self.checkpoint(val_loss, model)  #ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¦ã‚¹ã‚³ã‚¢è¡¨ç¤º\n",
    "            self.counter = 0  #ã‚¹ãƒˆãƒƒãƒ—ã‚«ã‚¦ãƒ³ã‚¿ãƒªã‚»ãƒƒãƒˆ\n",
    "\n",
    "    def checkpoint(self, val_loss, model):\n",
    "        '''ãƒ™ã‚¹ãƒˆã‚¹ã‚³ã‚¢æ›´æ–°æ™‚ã«å®Ÿè¡Œã•ã‚Œã‚‹ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆé–¢æ•°'''\n",
    "        if self.verbose:  #è¡¨ç¤ºã‚’æœ‰åŠ¹ã«ã—ãŸå ´åˆã¯ã€å‰å›ã®ãƒ™ã‚¹ãƒˆã‚¹ã‚³ã‚¢ã‹ã‚‰ã©ã‚Œã ã‘æ›´æ–°ã—ãŸã‹ï¼Ÿã‚’è¡¨ç¤º\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        save_path = os.path.join(args.model_save_path, f'{args.save_model_file_name}.pth')\n",
    "        torch.save(model.state_dict(), save_path)  #ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®šã—ãŸpathã«ä¿å­˜\n",
    "        self.val_loss_min = val_loss  #ãã®æ™‚ã®lossã‚’è¨˜éŒ²ã™ã‚‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Main ğŸƒâ€â™€ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check GPU\n",
      "You can use GPU(NVIDIA GeForce RTX 3090)\n",
      "-----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded weight from Checkpoints\\model_data40000.pth\n",
      "\n",
      "Starting training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 324.218360).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (324.218360 --> 169.036024).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (169.036024 --> 152.326215).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (152.326215 --> 131.481030).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (131.481030 --> 126.552311).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (126.552311 --> 117.718289).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (117.718289 --> 115.689377).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (115.689377 --> 110.436106).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (110.436106 --> 105.967190).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (105.967190 --> 93.573546).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (93.573546 --> 93.209206).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (93.209206 --> 90.725541).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (90.725541 --> 84.421769).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (84.421769 --> 83.112951).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (83.112951 --> 81.272874).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (81.272874 --> 81.270854).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (81.270854 --> 80.083955).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (80.083955 --> 78.692370).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (78.692370 --> 75.740050).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (75.740050 --> 74.847233).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (74.847233 --> 73.263275).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (73.263275 --> 72.771022).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (72.771022 --> 72.106287).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (72.106287 --> 71.147281).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (71.147281 --> 70.097331).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (70.097331 --> 68.078625).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (68.078625 --> 66.090409).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (66.090409 --> 65.094765).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (65.094765 --> 63.888591).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (63.888591 --> 63.498827).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (63.498827 --> 63.204927).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (63.204927 --> 61.825383).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (61.825383 --> 61.502147).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (61.502147 --> 60.804119).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (60.804119 --> 59.493612).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (59.493612 --> 58.888947).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (58.888947 --> 58.275970).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# æ™‚é–“è¨ˆæ¸¬é–‹å§‹\n",
    "time_start = time.time()\n",
    "\n",
    "#GPU\n",
    "device = check_GPU()\n",
    "\n",
    "os.makedirs(args.model_save_path, exist_ok=True) #é‡ã¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª\n",
    "# å­¦ç¿’æ¡ä»¶ã®ä¿å­˜\n",
    "csv_file = os.path.join(args.model_save_path,f\"{args.save_model_file_name}_info.csv\")\n",
    "with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    saver = csv.writer(file)\n",
    "    # ãƒ˜ãƒƒãƒ€ãƒ¼ï¼ˆè¾æ›¸ã®ã‚­ãƒ¼ï¼‰ã‚’æ›¸ãè¾¼ã‚€\n",
    "    saver.writerow(args.keys())\n",
    "    # ãƒ‡ãƒ¼ã‚¿ï¼ˆè¾æ›¸ã®å€¤ï¼‰ã‚’æ›¸ãè¾¼ã‚€\n",
    "    saver.writerow(args.values())\n",
    "\n",
    "\n",
    "# #train test split  \n",
    "# dataset_dict = image_label_collector()\n",
    "\n",
    "train_size = find_closest_divisible(args.train_size, int(args.batch_size))\n",
    "valid_size = find_closest_divisible(args.valid_size, int(args.batch_size))\n",
    "\n",
    "# Quick start (train test splitã‚’ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã—ã¦ä½¿ã†)\n",
    "train_df = pd.read_csv(\"train.csv\")[:train_size]\n",
    "valid_df = pd.read_csv(\"valid.csv\")[:valid_size]\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "dataset_dict = {\n",
    "        \"train\":train_df,\n",
    "        \"valid\":valid_df,\n",
    "        \"test\":test_df\n",
    "    }\n",
    "\n",
    "#ç”»åƒã®æ¨™æº–åŒ–\n",
    "mean, std = image_normalization()\n",
    "\n",
    "transform = ImageTransform(mean, std)\n",
    "\n",
    "#logger\n",
    "writer = tbx.SummaryWriter()\n",
    "\n",
    "# Dataset\n",
    "train_dataset = Image_Dataset(df=dataset_dict[\"train\"], transform=transform, phase='train') \n",
    "valid_dataset = Image_Dataset(df=dataset_dict[\"valid\"], transform=transform, phase='valid')\n",
    "\n",
    "# make dataloader\n",
    "train_dataloader=torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    pin_memory=True,\n",
    "    shuffle=True, # è¨“ç·´æ™‚ã¨ã¯é•ã„ãƒ©ãƒ³ãƒ€ãƒ ã«ã—ãŸ\n",
    "    )\n",
    "\n",
    "valid_dataloader=torch.utils.data.DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    pin_memory=True,\n",
    "    shuffle=False,\n",
    "    )\n",
    "\n",
    "# è¾æ›¸å‹å¤‰æ•°ã«ã¾ã¨ã‚ã‚‹\n",
    "dataloaders_dict = {\"train\": train_dataloader, \"valid\": valid_dataloader}          \n",
    "\n",
    "#Network\n",
    "model = ResNet(block)\n",
    "# load weights\n",
    "load_path = os.path.join(args.model_save_path,args.load_model_file_name+\".pth\")\n",
    "model.load_state_dict(torch.load(load_path))\n",
    "print(f\"Loaded weight from {load_path}\")\n",
    "model.to(device)\n",
    "\n",
    "#criterion\n",
    "criterion = torch.nn.MSELoss()\n",
    "#optimizer\n",
    "opt = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "#early topping \n",
    "earlystopping = EarlyStopping(patience=args.early_stop_patience, verbose=True) \n",
    "\n",
    "#train\n",
    "train(model, dataloaders_dict, device, opt, criterion, writer, earlystopping)\n",
    "\n",
    "writer.close()\n",
    "\n",
    "#æ™‚é–“è¨ˆæ¸¬çµ‚äº†\n",
    "time_end = time.time()\n",
    "#çµŒéæ™‚é–“ï¼ˆç§’ï¼‰\n",
    "elapsed = time_end - time_start\n",
    "#ç§’â¡æ™‚é–“\n",
    "td = datetime.timedelta(seconds=elapsed)\n",
    "#çµŒéæ™‚é–“è¨˜æ†¶\n",
    "f = open('elapsed_time.txt','a')\n",
    "f.write(f\"{args.save_model_file_name}.pth : {str(td)} ,\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   \n",
    "    ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€{ãƒ½ï¾âˆ§\n",
    "    ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ ã€€ å½¡å½¡ ã€€ .ï¼¼\n",
    "    ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ ã€€ å½¡å½¡ã€€ã€€ã€€ .â— ãƒ½\n",
    "    ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ å½¡ã€€ã€€ ï¼ˆã€€ã€€ã€€ã€€ï¼¼\n",
    "    ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ ï¼¿â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ï¼¿ï¼¿ï¼¿ï¼¿â”€â”€../ã€€             /ãƒ½ ã€€ ãƒ½ã€€ã€€ ï¼ï¿£ï¿£ï¿£ï¿£ï¿£ï¿£ï¿£ï¿£\n",
    "    ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ï¼ã€€ã€€ã€€ ã€€ ã€€ /ã€€ ã€€ ã€€ ./ã€€ã€€ã€€ã€€ã€€ã€€ ã€€ ./ã€€ã€€ãƒ½ã€€oä¸¿ã€€ï¼œã€€ãƒ¯ãƒ¼ãƒ—ã«ä½¿ã£ã¦ãã ã•ã„\n",
    "    ã€€ ã€€ ã€€ ã€€ ã€€ ã€€ ã€€ å½¡å½¡å½¡ã€€ ã€€ ã€€ ã€€ ã€€ ./ã€€ã€€ã€€ã€€ã€€/ã€€ã€€ã€€ã€€ã€€ ã€€ ã€€  â”‚ ã€€ ã€€ ã€€ ã€€ ã€€ï¼¼ï¼¿ï¼¿ï¼¿ï¼¿ï¼¿ï¼¿ï¼¿ï¼¿\n",
    "    ã€€ã€€ ã€€ ã€€ ã€€ ã€€ å½¡å½¡å½¡å½¡ã€€ã€€ã€€ ã€€ ã€€ ã€€  |ï½ºï¾ï½°ï¾™ï¾„ï¾ï½¼ï½¯ï¾Œï¾Ÿ/ã€€ã€€ã€€ã€€ã€€ã€€ ã€€ ã€€ â”‚\n",
    "    ã€€ã€€ ã€€ ã€€ å½¡å½¡å½¡ã€€ã€€ã€€ /ã€€ã€€ã€€ã€€ã€€ã€€ã€€ ..ï¿£ï¿£ï¿£ï¿£ï¿£ã€€ .ãƒ½ã€ã€€ ã€€ ã€€ ãƒ½ã€€ï¾‰\n",
    "    ã€€ã€€ ã€€ å½¡å½¡ã€€ã€€ã€€ã€€ ã€€ ï¾‰ã€€ ã€€ ã€€ ï¼ï¿£ï¿£ï½€ ãƒ½ ï½¤ã€€ã€€ ã€€ ã€€ ã€€ ï½€ï½¤ã€€ã€€ ã€€ ï¾‰ã€€ï¼¼\n",
    "    ã€€ã€€ã€€ å½¡ã€€ã€€ã€€ã€€ã€€ã€€ ï¼ã€€ã€€ ï¼ã€€ï½€ï½¤ã€€ã€€ã€€ï¼ã€€ã€€ ï½€ã€€ãƒ¼ ï½¤ ï¼¿ï¼¿ãƒ½ã€€ã€€ ãƒ½ã€€ ã€€  ãƒ½ ã€\n",
    "    ã€€ã€€ã€€ã€€ã€€ ã€€ ã€€ ã€€ ï¼ ã€€ ï¼ã€€ã€€ã€€ /ã€€ã€€ /ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ ã€€ ã€€ ã€€ ï¼¼ã€€ ä¸¶ã€€- ï½¤ ï½€ï½¤\n",
    "    ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€/ã€€.ï¼ã€€ã€€ã€€ã€€ï¼ ã€€ ï¼ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ ï¼¼ã€€ï½€ï½¤ã€€ã€€ï¼¼ï¼¼\n",
    "    ã€€ã€€ã€€ã€€ ã€€ ã€€ .ï¼.ï¼ã€€ã€€ã€€ã€€ã€€ |ã€€ã€€/ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ ã€€  ï¼¼ ä¸¶ ã€€ ã€€ãƒ½ ãƒ½\n",
    "    ã€€ã€€ã€€ ã€€ ___ï¼.ï¼ã€€ ã€€ ã€€ ã€€ ã€€ |ã€€|ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ãƒ½ï½€ï½¤ ã€€ ã€€ï½€ï½¤ï½€ï½¤\n",
    "    ã€€ã€€ã€€ã€€/ |__ï¼ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ | |ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ï½€ï½¤ãƒ½ã€ã€€ã€€ ï½”ï¾†ã‚\n",
    "    ã€€ã€€ ã€€ ï¿£ã€€ã€€ ã€€ ã€€ ã€€ ã€€ ã€€ \"\"\"\"''\"\"'\"\" ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ ã€€ ã€€ ã€€ ãƒ½ ï½¤ãƒ½ã€€ã€€ã€€\n",
    "    ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ ã€€ ã€€ ã€€ ã€€ ã€€ ï½”ï¾†ã‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã«å…¥åŠ›ã—ã¦ï¼Œãƒ­ã‚°ã‚’è¦‹ã‚‹  \n",
    "conda activate env_pytorch  \n",
    "tensorboard --logdir runs  \n",
    "çµ‚ã‚ã‚‹ã¨ãâ¡ctrlã‚­ãƒ¼ã¨cã‚’åŒæ™‚ã«å…¥åŠ›  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
