{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries ğŸ“š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import easydict\n",
    "from glob import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch.utils.data as data\n",
    "from tqdm import tqdm\n",
    "from natsort import natsorted\n",
    "from glob import glob\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "import warnings\n",
    "import tensorboardX as tbx\n",
    "import csv\n",
    "\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random seedğŸŒ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#å†ç¾æ€§ã®ç¢ºä¿\n",
    "seed = 44\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    #Disabling the benchmark by CUDA convolution operation(GPUã‚’ä½¿ã†ã¨ãã®å†ç¾æ€§ã®æ‹…ä¿) (https://pytorch.org/docs/stable/notes/randomness.html)\n",
    "    torch.backends.cudnn.benchmark = False \n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments ğŸ“—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict({\n",
    "    \"batch_size\":32, # ä½•æšåŒæ™‚ã«ãƒ¢ãƒ‡ãƒ«ã‚’ç”»åƒã«å…¥åŠ›ã™ã‚‹ã‹\n",
    "    \"epochs\":100000, #ã‚¢ãƒ¼ãƒªãƒ¼ã‚¹ãƒˆãƒƒãƒ”ãƒ³ã‚°ã®\n",
    "    \"learning_rate\" : 0.0001, # æå¤±ãŒä¸‹ãŒã‚Šãã‚‰ãªã„å ´åˆã¯ä¸‹ã’ã‚‹ã¨ã„ã„ã‹ã‚‚ï¼Ÿã€€adamã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ0.001\n",
    "    \"early_stop_patience\" : 25,\n",
    "\n",
    "    \"label_path\" : r\"E:\\2023_3_chuouseiki_WorkingDistance\\resize224\\L*mm\\GroundTruth_ZeroEdge_Cutoff10%.csv\", #æ­£è§£ãƒ©ãƒ™ãƒ«csvãƒ•ã‚¡ã‚¤ãƒ«\n",
    "    \n",
    "    \"model_save_path\" : \"Checkpoints\",\n",
    "    \"model_file_name\" : \"model_data40000\",\n",
    "    \n",
    "    \"train_size\" : 40000,\n",
    "    \"valid_size\" : 10000,\n",
    "    \n",
    "    #æå¤±é–¢æ•°ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "    \"tau\" : 0.5, # æ¸©åº¦Ï„ãŒå°ã•ã„ã»ã©æ­£ãƒ»è² ã®ã‚µãƒ³ãƒ—ãƒ«é–“ã®å·®ãŒå¤§ãããªã‚Šã€Ï„ãŒå¤§ãããªã‚‹ã¨ã€æ­£ã®é¡ä¼¼åº¦ã¯1ã«è¿‘ã¥ãã€æ­£ãƒ»è² ã®ã‚µãƒ³ãƒ—ãƒ«é–“ã®å·®ã¯å°ã•ããªã‚Šã¾ã™ã€‚\n",
    "    \"tau2\" : 0.5, #å°ã•ã„ã¨ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆãŒä»˜ãã‚„ã™ããªã‚‹ãŒï¼Œç„¡é™ã«ç™ºæ•£ã™ã‚‹ã“ã¨ã«æ³¨æ„.\n",
    "    \"gamma\" : 1, #å›å¸°æå¤±ã‚’é‡è¦–ã™ã‚‹å‰²åˆ,\n",
    "    \"note\": \"ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆ:å›å¸°=?:1\",\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU ğŸ“º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_GPU():\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒã‚¤ã‚¹ã‚’å‡ºåŠ›ã™ã‚‹é–¢æ•°\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    device : object\n",
    "        GPUãŒä½¿ãˆã‚‹ãªã‚‰'cuda:0',ä½¿ãˆãªã„ãªã‚‰'cpu'ã‚’è¿”ã™\n",
    "    \"\"\"\n",
    "    print(\"Check GPU\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"You can use GPU({torch.cuda.get_device_name()})\")\n",
    "        d_type = \"cuda:0\"\n",
    "    else:\n",
    "        print(\"You use cpu\")\n",
    "        d_type = \"cpu\"\n",
    "    print(\"-----\")\n",
    "    device = torch.device(d_type)\n",
    "    return device "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image & Label ğŸ“‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_divisible(a:int, b:int):\n",
    "    \"\"\"\n",
    "    ãƒãƒƒãƒã‚µã‚¤ã‚ºã§å‰²ã‚Šåˆ‡ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿æ•°ã‚’æ±‚ã‚ã‚‹ãŸã‚ã®é–¢æ•°\n",
    "    \"\"\"\n",
    "    remainder = a % b\n",
    "    if remainder == 0:\n",
    "        return int(a)  # aã¯ã™ã§ã«bã§å‰²ã‚Šåˆ‡ã‚Œã‚‹\n",
    "    elif remainder >= b / 2:\n",
    "        return int(a + (b - remainder))  # aã‚’å¢—ã‚„ã—ã¦bã®æ¬¡ã®å€æ•°ã«ã™ã‚‹\n",
    "    else:\n",
    "        return int(a - remainder)  # aã‚’æ¸›ã‚‰ã—ã¦bã®ç¾åœ¨ã®å€æ•°ã«ã™ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_label_collector(args=args):\n",
    "\n",
    "    \"\"\"\n",
    "    ç”»åƒã¨ãƒ©ãƒ™ãƒ«ã‚’é›†ã‚ã‚‹é–¢æ•°\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    dataset_dict:dict\n",
    "    \"\"\"\n",
    "\n",
    "    paths = natsorted(glob(args.label_path))\n",
    "    df = pd.DataFrame()\n",
    "    for path in tqdm(paths, desc=\"preparing data...\", leave=False):\n",
    "        df_ = pd.read_csv(path)\n",
    "        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«WDè¿½åŠ \n",
    "        l = int(re.findall(\"L(\\d+)mm\",path)[0])\n",
    "        df_[\"l\"] = l\n",
    "        # æºã‚‰ãé‡(æ°´å¹³ï¼‹å‚ç›´)\n",
    "        df_[\"d\"] = np.sqrt(df_[\"dx\"]**2 + df_[\"dy\"]**2) \n",
    "        # æºã‚‰ãé‡é †ã«ã‚½ãƒ¼ãƒˆ\n",
    "        df_.sort_values(by=\"d\", inplace=True)\n",
    "        # çµåˆ\n",
    "        df = pd.concat([df,df_])\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # ãƒ‡ãƒ¼ã‚¿ï¼’åˆ†å‰²\n",
    "    df1 = df[::2]\n",
    "    df2 = df[1::2]\n",
    "\n",
    "    # å…¨ãƒ‡ãƒ¼ã‚¿æ•°\n",
    "    all_data_num = len(df1)\n",
    "    # WDã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åˆ†ã‘ã—ãŸã¨ãã®1ã‚°ãƒ«ãƒ¼ãƒ—ã‚ãŸã‚Šã®ãƒ‡ãƒ¼ã‚¿æ•°\n",
    "    each_group_num = int(all_data_num//(args.batch_size/2))\n",
    "    # ä½™ã‚Š\n",
    "    reminder_num = int(all_data_num%(args.batch_size/2))\n",
    "    # ä½™ã‚Šåˆ†ã‚’è¿‘è·é›¢ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å‰Šé™¤\n",
    "    df1 = df1[reminder_num:].reset_index(drop=True)\n",
    "    df2 = df2[reminder_num:].reset_index(drop=True)\n",
    "    # WDã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åˆ†ã‘\n",
    "    df1[\"l_group\"] = 0\n",
    "    df2[\"l_group\"] = 0\n",
    "    for i in range(int(args.batch_size/2)):\n",
    "        df1[\"l_group\"][each_group_num*i:each_group_num*(i+1)] = i\n",
    "        df2[\"l_group\"][each_group_num*i:each_group_num*(i+1)] = i\n",
    "\n",
    "    #ã€€ãƒ‡ãƒ¼ã‚¿æ•°ã‚’ãƒãƒƒãƒã‚µã‚¤ã‚ºã§å‰²ã‚Šåˆ‡ã‚Œã‚‹æ•°ã«ã™ã‚‹\n",
    "    train_data_num = find_closest_divisible(args.train_size, int(args.batch_size/2))\n",
    "    valid_data_num = find_closest_divisible(args.valid_size, int(args.batch_size/2))\n",
    "\n",
    "    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿æŠ½é¸\n",
    "    train_df1 = df1.sample(train_data_num,random_state=seed)\n",
    "    train_df2 = df2.sample(train_data_num,random_state=seed)\n",
    "    # ã‚µãƒ³ãƒ—ãƒ«ã—ãŸè¡Œã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—.df1ã¨df2ã¯indexãŒä¸€è‡´ã—ã¦ã‚‹\n",
    "    sampled_indexes = train_df1.index\n",
    "    # å…ƒã®DataFrameã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«ã—ãŸè¡Œã‚’å‰Šé™¤\n",
    "    df1 = df1.drop(sampled_indexes)\n",
    "    df2 = df2.drop(sampled_indexes)\n",
    "    # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿æŠ½å‡º\n",
    "    valid_df1 = df1.sample(valid_data_num,random_state=seed)\n",
    "    valid_df2 = df2.sample(valid_data_num,random_state=seed)\n",
    "    # ã‚µãƒ³ãƒ—ãƒ«ã—ãŸè¡Œã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—.df1ã¨df2ã¯indexãŒä¸€è‡´ã—ã¦ã‚‹\n",
    "    sampled_indexes = valid_df1.index\n",
    "    # å…ƒã®DataFrameã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«ã—ãŸè¡Œã‚’å‰Šé™¤\n",
    "    df1 = df1.drop(sampled_indexes)\n",
    "    df2 = df2.drop(sampled_indexes)\n",
    "    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã—ã€ã‚·ãƒ¼ãƒ‰å€¤ã‚’å›ºå®š\n",
    "    df1 = df1.sample(frac=1, random_state=seed)\n",
    "    df2 = df2.sample(frac=1, random_state=seed)\n",
    "\n",
    "    # äº¤äº’ã«(args.batch_size/2)è¡Œãšã¤ãƒ‡ãƒ¼ã‚¿ã‚’å–ã‚Šå‡ºã—ã€çµåˆã™ã‚‹\n",
    "    train_df = pd.DataFrame()\n",
    "    for i in tqdm(range(0, len(train_df1), int(args.batch_size/2)), desc=\"sorting train data...\", leave=False):\n",
    "        train_df = pd.concat([train_df, train_df1[i:i+int(args.batch_size/2)], train_df2[i:i+int(args.batch_size/2)]])\n",
    "    valid_df = pd.DataFrame()\n",
    "    for i in tqdm(range(0, len(valid_df1), int(args.batch_size/2)), desc=\"sorting valid data...\", leave=False):\n",
    "        valid_df = pd.concat([valid_df, valid_df1[i:i+int(args.batch_size/2)], valid_df2[i:i+int(args.batch_size/2)]])\n",
    "    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¯ãã®ã¾ã¾çµåˆ\n",
    "    test_df = pd.concat([df1,df2])\n",
    "\n",
    "    # è¾æ›¸å‹ã«ã¾ã¨ã‚ã‚‹ï¼\n",
    "    dataset_dict = {\n",
    "        \"train\" : train_df,\n",
    "        \"valid\" : valid_df,\n",
    "        \"test\" : test_df,\n",
    "    }\n",
    "\n",
    "    # ä¿å­˜\n",
    "    for key, value in dataset_dict.items():\n",
    "        value.to_csv(f\"{key}.csv\",index=False)\n",
    "    \n",
    "    return dataset_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Normalization ğŸ“Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_normalization(train_df, num=500):\n",
    "    \"\"\"\n",
    "    ç”»åƒã®æ¨™æº–åŒ–ã‚’è¡Œã†ãŸã‚ã«ãƒ‡ãƒ¼ã‚¿ã®å¹³å‡ã¨æ¨™æº–åå·®ã‚’æ±‚ã‚ã‚‹é–¢æ•°\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img_path_list : list\n",
    "        globã§é›†ã‚ãŸç”»åƒpath\n",
    "    num : int\n",
    "        å¹³å‡ã¨æ¨™æº–åå·®ã‚’æ±‚ã‚ã‚‹ã®ã«ä½¿ã†ç”»åƒæšæ•° \n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    mean : tuple\n",
    "        ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç”»ç´ ã®å¹³å‡\n",
    "    std : tuple\n",
    "        ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç”»ç´ ã®æ¨™æº–åå·®    \n",
    "    \"\"\"\n",
    "\n",
    "    tensors = []\n",
    "    to_tensor = transforms.ToTensor()\n",
    "    sample_df = train_df.sample(num)\n",
    "    img_path_list = sample_df[\"fname\"]\n",
    "    for img in img_path_list:\n",
    "        img = Image.open(img)\n",
    "        img = to_tensor(img)\n",
    "        tensors.append(img)\n",
    "    tensors = torch.stack([img_t for img_t in tensors], dim=3) #dimã¯ã©ã“ã§ã‚‚ã‚ˆã„\n",
    "    mean = tensors.view(1,-1).mean(dim=1).item()\n",
    "    std = tensors.view(1,-1).std(dim=1).item()\n",
    "    \n",
    "    df = pd.DataFrame({\"mean\":[mean],\n",
    "                       \"std\":[std]})\n",
    "    df.to_csv(\"mean_std.csv\")\n",
    "    \n",
    "    return mean, std    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data set ğŸ§°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTransform:\n",
    "    \"\"\"\n",
    "    ç”»åƒã®å‰å‡¦ç†ã‚¯ãƒ©ã‚¹\n",
    "    è¨“ç·´æ™‚ã ã‘ã€ãƒ‡ãƒ¼ã‚¿ã‚ªãƒ¼ã‚®ãƒ¥ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³(DA)ãŒã§ãã‚‹ã‚ˆã†ã«ã€train,validã§åˆ†ã‘ã¦æ›¸ã„ãŸ\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    mean : int\n",
    "        ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç”»ç´ å€¤ã®å¹³å‡å€¤\n",
    "    std : int\n",
    "        ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç”»ç´ å€¤ã®æ¨™æº–åå·®\n",
    "    \"\"\"   \n",
    "\n",
    "    def __init__(self, mean, std):\n",
    "        self.data_transform = {\n",
    "            'train':transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std) \n",
    "            ]),\n",
    "            'valid':transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std)\n",
    "            ])\n",
    "        }\n",
    "    \n",
    "    #å¼•æ•°ãªã—ã§å‘¼ã°ã‚ŒãŸã¨ãã®æŒ™å‹•ã‚’å®šç¾©\n",
    "    def __call__(self, img, phase='train'):\n",
    "        return self.data_transform[phase](img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image_Dataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    ç”»åƒã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¯ãƒ©ã‚¹ã€‚\n",
    "    Pytorch Dataset class ã‚’ç¶™æ‰¿\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    file_list : list\n",
    "        ç”»åƒã®path_list\n",
    "    label_list : list\n",
    "        ãƒ©ãƒ™ãƒ«ã®path_list       \n",
    "    transform : object\n",
    "        class ImageTransform()    \n",
    "    phase : str\n",
    "        'train' or 'valid'\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df, transform, phase='train'):\n",
    "        self.file_list = df[\"fname\"] \n",
    "        self.labels = df[[\"dx\",\"dy\"]].to_numpy()\n",
    "        #self.l = df[\"l\"]\n",
    "        self.d = df[\"d\"]\n",
    "        self.transform = transform  \n",
    "        self.phase = phase  \n",
    "\n",
    "    #ã“ã®ã‚¯ãƒ©ã‚¹ã®ç”»åƒæšæ•°ã‚’å®šç¾©ã€‚\n",
    "    def __len__(self):\n",
    "        return len(self.file_list) \n",
    "\n",
    "    #ã“ã®ã‚¯ãƒ©ã‚¹ã«è§’æ‹¬å¼§ã§ã‚¢ã‚¯ã‚»ã‚¹ã—ãŸæ™‚ã®æŒ™å‹•ã‚’å®šç¾©\n",
    "    def __getitem__(self, index):\n",
    "        # ç”»åƒã‚’ãƒ­ãƒ¼ãƒ‰\n",
    "        img_path = self.file_list[index]\n",
    "        img = Image.open(img_path)\n",
    "        #img = img.convert(\"L\") #ã‚°ãƒ¬ã‚¤ã‚¹ã‚±ãƒ¼ãƒ« \n",
    "        # å‰å‡¦ç†\n",
    "        img_transformed = self.transform(\n",
    "            img, self.phase)  # torch.Size([width, height]) \n",
    "\n",
    "        #label\n",
    "        label = self.labels[index]\n",
    "        #Tensorã«å¤‰æ›\n",
    "        label = torch.tensor(label, dtype=torch.float32) \n",
    "\n",
    "        # ç©ºæ°—æºã‚‰ãé‡\n",
    "        d = self.d[index]           \n",
    "\n",
    "        return img_transformed, label, d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NetWork ğŸ§ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ãƒ¢ãƒ‡ãƒ«\n",
    "\n",
    "# # æ®‹å·®ãƒ–ãƒ­ãƒƒã‚¯\n",
    "class block(nn.Module):\n",
    "    def __init__(self, first_conv_in_channels, first_conv_out_channels, identity_conv=None, stride=1):\n",
    "        \"\"\"\n",
    "        æ®‹å·®ãƒ–ãƒ­ãƒƒã‚¯ã‚’ä½œæˆã™ã‚‹ã‚¯ãƒ©ã‚¹\n",
    "        Args:\n",
    "            first_conv_in_channels : 1ç•ªç›®ã®convå±¤ï¼ˆ1Ã—1ï¼‰ã®input channelæ•°\n",
    "            first_conv_out_channels : 1ç•ªç›®ã®convå±¤ï¼ˆ1Ã—1ï¼‰ã®output channelæ•°\n",
    "            identity_conv : channelæ•°èª¿æ•´ç”¨ã®convå±¤\n",
    "            stride : 3Ã—3convå±¤ã«ãŠã‘ã‚‹stideæ•°ã€‚sizeã‚’åŠåˆ†ã«ã—ãŸã„ã¨ãã¯2ã«è¨­å®š\n",
    "        \"\"\"        \n",
    "        super(block, self).__init__()\n",
    "\n",
    "        # 1ç•ªç›®ã®convå±¤ï¼ˆ1Ã—1ï¼‰\n",
    "        self.conv1 = nn.Conv2d(first_conv_in_channels, first_conv_out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(first_conv_out_channels)\n",
    "\n",
    "        # 2ç•ªç›®ã®convå±¤ï¼ˆ3Ã—3ï¼‰\n",
    "        # ãƒ‘ã‚¿ãƒ¼ãƒ³3ã®æ™‚ã¯sizeã‚’å¤‰æ›´ã§ãã‚‹ã‚ˆã†ã«strideã¯å¯å¤‰\n",
    "        self.conv2 = nn.Conv2d(first_conv_out_channels, first_conv_out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(first_conv_out_channels)\n",
    "\n",
    "        # 3ç•ªç›®ã®convå±¤ï¼ˆ1Ã—1ï¼‰\n",
    "        # output channelã¯input channelã®4å€ã«ãªã‚‹\n",
    "        self.conv3 = nn.Conv2d(first_conv_out_channels, first_conv_out_channels*4, kernel_size=1, stride=1, padding=0)\n",
    "        self.bn3 = nn.BatchNorm2d(first_conv_out_channels*4)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # identityã®channelæ•°ã®èª¿æ•´ãŒå¿…è¦ãªå ´åˆã¯convå±¤ï¼ˆ1Ã—1ï¼‰ã‚’ç”¨æ„ã€ä¸è¦ãªå ´åˆã¯None\n",
    "        self.identity_conv = identity_conv\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        identity = x.clone()  # å…¥åŠ›ã‚’ä¿æŒã™ã‚‹\n",
    "\n",
    "        x = self.conv1(x)  # 1Ã—1ã®ç•³ã¿è¾¼ã¿\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv2(x)  # 3Ã—3ã®ç•³ã¿è¾¼ã¿ï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³3ã®æ™‚ã¯strideãŒ2ã«ãªã‚‹ãŸã‚ã€ã“ã“ã§sizeãŒåŠåˆ†ã«ãªã‚‹ï¼‰\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.conv3(x)  # 1Ã—1ã®ç•³ã¿è¾¼ã¿\n",
    "        x = self.bn3(x)\n",
    "\n",
    "        # å¿…è¦ãªå ´åˆã¯convå±¤ï¼ˆ1Ã—1ï¼‰ã‚’é€šã—ã¦identityã®channelæ•°ã®èª¿æ•´ã—ã¦ã‹ã‚‰è¶³ã™\n",
    "        if self.identity_conv is not None:\n",
    "            identity = self.identity_conv(identity)\n",
    "        x += identity\n",
    "\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "#  Resnet50\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self,block):\n",
    "        super(ResNet,self).__init__()\n",
    "\n",
    "        # conv1ã¯ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£é€šã‚Šã«ãƒ™ã‚¿æ‰“ã¡\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        # self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1) # in:(64,112*112)ã€out:(64,56*56)\n",
    "        self.maxpool = nn.Identity()\n",
    "\n",
    "        # conv2_xã¯ã‚µã‚¤ã‚ºã®å¤‰æ›´ã¯ä¸è¦ã®ãŸã‚ã€strideã¯1\n",
    "        self.conv2_x = self._make_layer(block, 3, res_block_in_channels=64, first_conv_out_channels=64, stride=1)\n",
    "\n",
    "        # conv3_xä»¥é™ã¯ã‚µã‚¤ã‚ºã®å¤‰æ›´ã‚’ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ãŸã‚ã€strideã¯2\n",
    "        self.conv3_x = self._make_layer(block, 4, res_block_in_channels=256,  first_conv_out_channels=128, stride=2)\n",
    "        self.conv4_x = self._make_layer(block, 6, res_block_in_channels=512,  first_conv_out_channels=256, stride=2)\n",
    "        self.conv5_x = self._make_layer(block, 3, res_block_in_channels=1024, first_conv_out_channels=512, stride=2)\n",
    "\n",
    "        #self.avgpool = nn.AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        # self.fc1 = nn.Linear(512*4,2)\n",
    "        self.fc1 = nn.Identity()\n",
    "    \n",
    "        # mlp projection head\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(in_features=2048, out_features=2048),\n",
    "            nn.BatchNorm1d(2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=2048, out_features=128),\n",
    "            nn.BatchNorm1d(128),\n",
    "        )\n",
    "        # Regression\n",
    "        self.regression = nn.Sequential(\n",
    "            nn.Linear(2048, 512), \n",
    "            nn.LeakyReLU(),       \n",
    "            nn.Linear(512, 128),  \n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, 32),        \n",
    "            nn.Linear(32, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        x = self.conv1(x)  \n",
    "        x = self.bn1(x)     \n",
    "        x = self.relu(x)    \n",
    "        x = self.maxpool(x) #out:(64*16*16)\n",
    "        \n",
    "\n",
    "        x = self.conv2_x(x)  # in:(64,56*56)  ã€out:(256,56*56)\n",
    "        x = self.conv3_x(x)  # in:(256,56*56) ã€out:(512,28*28)\n",
    "        x = self.conv4_x(x)  # in:(512,28*28) ã€out:(1024,14*14)\n",
    "        x = self.conv5_x(x)  # in:(1024,14*14)ã€out:(2048,7*7)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.reshape(x.shape[0], -1) # 2048\n",
    "        x = self.fc1(x)\n",
    "        #backbone_output = x.clone()\n",
    "\n",
    "        projection_output = self.projection(x)\n",
    "        regression_output = self.regression(x)\n",
    "\n",
    "        return projection_output, regression_output\n",
    "\n",
    "    def _make_layer(self, block, num_res_blocks, res_block_in_channels, first_conv_out_channels, stride):\n",
    "        layers = []\n",
    "\n",
    "        # 1ã¤ç›®ã®æ®‹å·®ãƒ–ãƒ­ãƒƒã‚¯ã§ã¯channelèª¿æ•´ã€åŠã³sizeèª¿æ•´ãŒç™ºç”Ÿã™ã‚‹\n",
    "        # identifyã‚’è¶³ã™å‰ã«1Ã—1ã®convå±¤ã‚’è¿½åŠ ã—ã€ã‚µã‚¤ã‚ºèª¿æ•´ãŒå¿…è¦ãªå ´åˆã¯strideã‚’2ã«è¨­å®š\n",
    "        identity_conv = nn.Conv2d(res_block_in_channels, first_conv_out_channels*4, kernel_size=1,stride=stride)\n",
    "        layers.append(block(res_block_in_channels, first_conv_out_channels, identity_conv, stride))\n",
    "\n",
    "        # 2ã¤ç›®ä»¥é™ã®input_channelæ•°ã¯1ã¤ç›®ã®output_channelã®4å€\n",
    "        in_channels = first_conv_out_channels*4\n",
    "\n",
    "        # channelèª¿æ•´ã€sizeèª¿æ•´ã¯ç™ºç”Ÿã—ãªã„ãŸã‚ã€identity_convã¯Noneã€strideã¯1\n",
    "        for i in range(num_res_blocks - 1):\n",
    "            layers.append(block(in_channels, first_conv_out_channels, identity_conv=None, stride=1))\n",
    "\n",
    "        return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similar matrix ğŸ­"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é¡ä¼¼è¡Œåˆ—\n",
    "def sim_matrix(output1:torch.tensor, output2:torch.tensor):\n",
    "\n",
    "    \"\"\"\n",
    "    Attributes\n",
    "    ----------\n",
    "    output1 : torch.tensor\n",
    "        ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å‡ºåŠ›å‰åŠ\n",
    "    output2 : torch.tensor\n",
    "        ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å‡ºåŠ›å¾ŒåŠ\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    sim : torch.tensor\n",
    "        é¡ä¼¼è¡Œåˆ—\n",
    "    \"\"\"   \n",
    "    # å®Ÿã¯2ã¤ã«åˆ†ã‘ã‚‹æ„å‘³ãªã„\n",
    "    zi_norm = F.normalize(output1, dim=1)\n",
    "    zj_norm = F.normalize(output2, dim=1)\n",
    "    representation = torch.cat([zi_norm,zj_norm], dim=0)\n",
    "    sim = torch.matmul(representation, torch.t(representation))\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function ğŸŒˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrast_loss_function(sim, ds, device, args=args):\n",
    "    harf_batch_size = int(args.batch_size/2)\n",
    "\n",
    "    # è¡Œåˆ—å…¨ä½“ã«exp(./tau)ã‚’ã‹ã‘ã‚‹\n",
    "    sim = torch.exp(sim/args.tau)\n",
    "    \n",
    "    # é‡ã¿\n",
    "    # ç©ºæ°—æºã‚‰ãé‡ã®å·®\n",
    "    ds = ds.to(torch.float)\n",
    "    matrix1 = ds.repeat(args.batch_size,1)\n",
    "    matrix2 = matrix1.T\n",
    "    weight = torch.abs(matrix2 - matrix1)\n",
    "    # ãŸã¾ãŸã¾æºã‚‰ãé‡ãŒé‡è¤‡ã™ã‚‹å ´åˆï¼Œ0ã®è¦ç´ ã‚’1ã«å¤‰æ›ã—ã¾ã™.\n",
    "    weight[weight == 0] = 1\n",
    "    # æŒ‡æ•°ã«ã™ã‚‹\n",
    "    weight = torch.exp(weight/args.tau2)\n",
    "    # æŒ‡ç¤ºé–¢æ•°\n",
    "    mask = (~torch.eye(args.batch_size, args.batch_size, dtype=bool)).float()\n",
    "    weight = weight * mask\n",
    "    weight = weight.to(device)\n",
    "    bottom = weight*sim\n",
    "\n",
    "    #å¯¾è§’æˆåˆ†ã‚’æŠ½å‡ºã€€positive pairã‚’æŠ½å‡º\n",
    "    sim_ij = torch.diag(bottom, harf_batch_size)\n",
    "    sim_ji = torch.diag(bottom, -harf_batch_size)\n",
    "    top = torch.cat([sim_ij, sim_ji], dim=0)\n",
    "\n",
    "    all_losses = - torch.log(top/torch.sum(bottom, dim = 1))\n",
    "    loss = torch.sum(all_losses)/ args.batch_size\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressison_loss_function(output, labels, args=args):\n",
    "    criterion = nn.MSELoss()\n",
    "    loss = args.gamma * criterion(output, labels) / args.batch_size\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train ğŸ‹ï¸â€â™‚ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloaders_dict, device, opt, writer, earlystopping):\n",
    "\n",
    "    #global epoch\n",
    "\n",
    "    BREAK = False\n",
    "    print(f\"\\nStarting training\")\n",
    "    for epoch in range(0, args.epochs+1):\n",
    "\n",
    "        # epochã”ã¨ã®å­¦ç¿’ã¨æ¤œè¨¼ã®ãƒ«ãƒ¼ãƒ—\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ãƒ¢ãƒ¼ãƒ‰ã«\n",
    "            else:\n",
    "                model.eval()   # ãƒ¢ãƒ‡ãƒ«ã‚’æ¤œè¨¼ãƒ¢ãƒ¼ãƒ‰ã«\n",
    "\n",
    "            epoch_loss = 0.0  # epochã®æå¤±å’Œ\n",
    "            epoch_contrast_loss = 0.0\n",
    "            epoch_regression_loss = 0.0                \n",
    "\n",
    "            #æœªè¨“ç·´æ™‚ã®æ€§èƒ½è©•ä¾¡\n",
    "            if (epoch == 0 and phase=='train'):\n",
    "                continue\n",
    "            \n",
    "            # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‹ã‚‰ãƒŸãƒ‹ãƒãƒƒãƒã‚’å–ã‚Šå‡ºã™ãƒ«ãƒ¼ãƒ—ã€‚args['batch_size']æšã”ã¨å–ã‚Šå‡ºã™\n",
    "            for inputs, labels, ds in tqdm(dataloaders_dict[phase], leave=False, desc='Epoch {}/{} {}'.format(epoch, args.epochs, phase)): \n",
    "\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # optimizerã‚’åˆæœŸåŒ–\n",
    "                opt.zero_grad()\n",
    "\n",
    "                # é †ä¼æ¬ï¼ˆforwardï¼‰è¨ˆç®—\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    projection_output, regression_output = model(inputs)\n",
    "                    output1, output2 = torch.chunk(projection_output, chunks=2, dim=0)\n",
    "                    sim = sim_matrix(output1, output2)\n",
    "                    contrast_loss = contrast_loss_function(sim, ds, device)\n",
    "                    regression_loss = regressison_loss_function(regression_output, labels)\n",
    "                    loss = contrast_loss + regression_loss\n",
    "\n",
    "                    # è¨“ç·´æ™‚ã¯ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ‘ã‚²ãƒ¼ã‚·ãƒ§ãƒ³\n",
    "                    if phase == 'train':\n",
    "                        loss.backward() \n",
    "                        opt.step()\n",
    "\n",
    "                # æå¤±ã®è¨ˆç®—\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_contrast_loss += contrast_loss.item()\n",
    "                epoch_regression_loss += regression_loss.item()\n",
    "\n",
    "            #tensorboardã«å‡ºåŠ›\n",
    "            if phase == 'train':\n",
    "                writer.add_scalar('train_epoch_loss', epoch_loss, epoch) #(ã‚°ãƒ©ãƒ•å, yåº§æ¨™, xåº§æ¨™) \n",
    "                writer.add_scalar('train_contrast_epoch_loss', epoch_contrast_loss, epoch)    \n",
    "                writer.add_scalar('train_epoch_regression_loss', epoch_regression_loss, epoch)           \n",
    "            elif phase == 'valid':\n",
    "                writer.add_scalar('valid_epoch_loss', epoch_loss, epoch)\n",
    "                writer.add_scalar('valid_contrast_epoch_loss', epoch_contrast_loss, epoch)    \n",
    "                writer.add_scalar('valid_epoch_regression_loss', epoch_regression_loss, epoch)                 \n",
    "                #æ¯ã‚¨ãƒãƒƒã‚¯earlystoppingã®åˆ¤å®šã‚’ã•ã›ã‚‹\n",
    "                earlystopping(epoch_loss, model) #callãƒ¡ã‚½ãƒƒãƒ‰å‘¼ã³å‡ºã—\n",
    "                if earlystopping.early_stop: #ã‚¹ãƒˆãƒƒãƒ—ãƒ•ãƒ©ã‚°ãŒTrueã®å ´åˆã€breakã§forãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹\n",
    "                    print(\"Early Stopping!\")\n",
    "                    BREAK = True\n",
    "        if BREAK:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EarlyStopping â›”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    earlystoppingã‚¯ãƒ©ã‚¹\n",
    "    æå¤±ãŒä¸‹ãŒã£ãŸã“ã¨ã‚’åˆ¤æ–­ã—ã¦å­¦ç¿’ã‚’æ‰“ã¡åˆ‡ã‚‹\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    patience : int\n",
    "        ä½•å›æå¤±ãŒä¸‹ãŒã‚‰ãªã‹ã£ãŸã‚‰å­¦ç¿’ã‚’æ‰“ã¡åˆ‡ã‚‹ã‹\n",
    "    verbose : bool\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, patience, verbose):\n",
    "        \"\"\"å¼•æ•° : æœ€å°å€¤ã®éæ›´æ–°æ•°ã‚«ã‚¦ãƒ³ã‚¿ã€è¡¨ç¤ºè¨­å®š\"\"\"\n",
    "\n",
    "        self.patience = patience    #è¨­å®šã‚¹ãƒˆãƒƒãƒ—ã‚«ã‚¦ãƒ³ã‚¿\n",
    "        self.verbose = verbose      #è¡¨ç¤ºã®æœ‰ç„¡\n",
    "        self.counter = 0            #ç¾åœ¨ã®ã‚«ã‚¦ãƒ³ã‚¿å€¤\n",
    "        self.best_score = None      #ãƒ™ã‚¹ãƒˆã‚¹ã‚³ã‚¢\n",
    "        self.early_stop = False     #ã‚¹ãƒˆãƒƒãƒ—ãƒ•ãƒ©ã‚°\n",
    "        self.val_loss_min = np.Inf   #å‰å›ã®ãƒ™ã‚¹ãƒˆã‚¹ã‚³ã‚¢è¨˜æ†¶ç”¨\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        \"\"\"\n",
    "        ç‰¹æ®Š(call)ãƒ¡ã‚½ãƒƒãƒ‰\n",
    "        å®Ÿéš›ã«å­¦ç¿’ãƒ«ãƒ¼ãƒ—å†…ã§æœ€å°lossã‚’æ›´æ–°ã—ãŸã‹å¦ã‹ã‚’è¨ˆç®—ã•ã›ã‚‹éƒ¨åˆ†\n",
    "        \"\"\"\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:  #1Epochç›®ã®å‡¦ç†\n",
    "            self.best_score = score   #1Epochç›®ã¯ãã®ã¾ã¾ãƒ™ã‚¹ãƒˆã‚¹ã‚³ã‚¢ã¨ã—ã¦è¨˜éŒ²ã™ã‚‹\n",
    "            self.checkpoint(val_loss, model)  #è¨˜éŒ²å¾Œã«ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¦ã‚¹ã‚³ã‚¢è¡¨ç¤ºã™ã‚‹\n",
    "        elif score < self.best_score:  # ãƒ™ã‚¹ãƒˆã‚¹ã‚³ã‚¢ã‚’æ›´æ–°ã§ããªã‹ã£ãŸå ´åˆ\n",
    "            self.counter += 1   #ã‚¹ãƒˆãƒƒãƒ—ã‚«ã‚¦ãƒ³ã‚¿ã‚’+1\n",
    "            # if self.verbose:  #è¡¨ç¤ºã‚’æœ‰åŠ¹ã«ã—ãŸå ´åˆã¯çµŒéã‚’è¡¨ç¤º\n",
    "            #     print(f'EarlyStopping counter: {self.counter} out of {self.patience}')  #ç¾åœ¨ã®ã‚«ã‚¦ãƒ³ã‚¿ã‚’è¡¨ç¤ºã™ã‚‹ \n",
    "            if self.counter >= self.patience:  #è¨­å®šã‚«ã‚¦ãƒ³ãƒˆã‚’ä¸Šå›ã£ãŸã‚‰ã‚¹ãƒˆãƒƒãƒ—ãƒ•ãƒ©ã‚°ã‚’Trueã«å¤‰æ›´\n",
    "                self.early_stop = True\n",
    "        else:  #ãƒ™ã‚¹ãƒˆã‚¹ã‚³ã‚¢ã‚’æ›´æ–°ã—ãŸå ´åˆ\n",
    "            self.best_score = score  #ãƒ™ã‚¹ãƒˆã‚¹ã‚³ã‚¢ã‚’ä¸Šæ›¸ã\n",
    "            self.checkpoint(val_loss, model)  #ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¦ã‚¹ã‚³ã‚¢è¡¨ç¤º\n",
    "            self.counter = 0  #ã‚¹ãƒˆãƒƒãƒ—ã‚«ã‚¦ãƒ³ã‚¿ãƒªã‚»ãƒƒãƒˆ\n",
    "\n",
    "    def checkpoint(self, val_loss, model):\n",
    "        '''ãƒ™ã‚¹ãƒˆã‚¹ã‚³ã‚¢æ›´æ–°æ™‚ã«å®Ÿè¡Œã•ã‚Œã‚‹ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆé–¢æ•°'''\n",
    "        if self.verbose:  #è¡¨ç¤ºã‚’æœ‰åŠ¹ã«ã—ãŸå ´åˆã¯ã€å‰å›ã®ãƒ™ã‚¹ãƒˆã‚¹ã‚³ã‚¢ã‹ã‚‰ã©ã‚Œã ã‘æ›´æ–°ã—ãŸã‹ï¼Ÿã‚’è¡¨ç¤º\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        save_path = os.path.join(args.model_save_path, f'{args.model_file_name}.pth')\n",
    "        torch.save(model.state_dict(), save_path)  #ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®šã—ãŸpathã«ä¿å­˜\n",
    "        self.val_loss_min = val_loss  #ãã®æ™‚ã®lossã‚’è¨˜éŒ²ã™ã‚‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Main ğŸƒâ€â™€ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check GPU\n",
      "You can use GPU(NVIDIA GeForce RTX 3090)\n",
      "-----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 5625.198720).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (5625.198720 --> 4910.085599).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (4910.085599 --> 4907.389169).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (4907.389169 --> 4889.638279).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (4889.638279 --> 4889.463548).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (4889.463548 --> 4873.651827).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (4873.651827 --> 4867.343175).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (4867.343175 --> 4861.617738).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (4861.617738 --> 4839.588843).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (4839.588843 --> 4832.111764).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (4832.111764 --> 4806.198966).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (4806.198966 --> 4801.864892).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (4801.864892 --> 4794.575014).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (4794.575014 --> 4785.322016).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (4785.322016 --> 4768.661429).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (4768.661429 --> 4751.131589).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (4751.131589 --> 4735.666577).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (4735.666577 --> 4726.380434).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (4726.380434 --> 4719.337009).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (4719.337009 --> 4700.149067).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (4700.149067 --> 4692.345592).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (4692.345592 --> 4653.800603).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (4653.800603 --> 4644.607992).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (4644.607992 --> 4644.354204).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (4644.354204 --> 4639.106564).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# æ™‚é–“è¨ˆæ¸¬é–‹å§‹\n",
    "time_start = time.time()\n",
    "\n",
    "#GPU\n",
    "device = check_GPU()\n",
    "\n",
    "os.makedirs(args.model_save_path, exist_ok=True) #é‡ã¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª\n",
    "# å­¦ç¿’æ¡ä»¶ã®ä¿å­˜\n",
    "csv_file = os.path.join(args.model_save_path,f\"{args.model_file_name}_info.csv\")\n",
    "with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    saver = csv.writer(file)\n",
    "    # ãƒ˜ãƒƒãƒ€ãƒ¼ï¼ˆè¾æ›¸ã®ã‚­ãƒ¼ï¼‰ã‚’æ›¸ãè¾¼ã‚€\n",
    "    saver.writerow(args.keys())\n",
    "    # ãƒ‡ãƒ¼ã‚¿ï¼ˆè¾æ›¸ã®å€¤ï¼‰ã‚’æ›¸ãè¾¼ã‚€\n",
    "    saver.writerow(args.values())\n",
    "\n",
    "\n",
    "#train test split  \n",
    "#dataset_dict = image_label_collector()\n",
    "    \n",
    "train_size = find_closest_divisible(args.train_size, int(args.batch_size))\n",
    "valid_size = find_closest_divisible(args.valid_size, int(args.batch_size))\n",
    "\n",
    "# Quick start (train test splitã‚’ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã—ã¦ä½¿ã†)\n",
    "train_df = pd.read_csv(\"train.csv\")[:train_size]\n",
    "valid_df = pd.read_csv(\"valid.csv\")[:valid_size]\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "dataset_dict = {\n",
    "        \"train\":train_df,\n",
    "        \"valid\":valid_df,\n",
    "        \"test\":test_df\n",
    "    }\n",
    "\n",
    "#ç”»åƒã®æ¨™æº–åŒ–\n",
    "mean, std = image_normalization(dataset_dict[\"train\"])\n",
    "\n",
    "transform = ImageTransform(mean, std)\n",
    "\n",
    "#logger\n",
    "writer = tbx.SummaryWriter()\n",
    "\n",
    "# Dataset\n",
    "train_dataset = Image_Dataset(df=dataset_dict[\"train\"], transform=transform, phase='train') \n",
    "valid_dataset = Image_Dataset(df=dataset_dict[\"valid\"], transform=transform, phase='valid')\n",
    "\n",
    "# make dataloader\n",
    "train_dataloader=torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    pin_memory=True,\n",
    "    shuffle=False,\n",
    "    )\n",
    "\n",
    "valid_dataloader=torch.utils.data.DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    pin_memory=True,\n",
    "    shuffle=False,\n",
    "    )\n",
    "\n",
    "# è¾æ›¸å‹å¤‰æ•°ã«ã¾ã¨ã‚ã‚‹\n",
    "dataloaders_dict = {\"train\": train_dataloader, \"valid\": valid_dataloader}          \n",
    "\n",
    "#Network\n",
    "model = ResNet(block)\n",
    "model.to(device)\n",
    "\n",
    "#optimizer\n",
    "opt = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "#early topping \n",
    "earlystopping = EarlyStopping(patience=args.early_stop_patience, verbose=True) \n",
    "\n",
    "#train\n",
    "train(model, dataloaders_dict, device, opt, writer, earlystopping)\n",
    "\n",
    "writer.close()\n",
    "\n",
    "#æ™‚é–“è¨ˆæ¸¬çµ‚äº†\n",
    "time_end = time.time()\n",
    "#çµŒéæ™‚é–“ï¼ˆç§’ï¼‰\n",
    "elapsed = time_end - time_start\n",
    "#ç§’â¡æ™‚é–“\n",
    "td = datetime.timedelta(seconds=elapsed)\n",
    "#çµŒéæ™‚é–“è¨˜æ†¶\n",
    "f = open('elapsed_time.txt','a')\n",
    "f.write(f\"{args.model_file_name}.pth : {str(td)} ,\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   \n",
    "    ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€{ãƒ½ï¾âˆ§\n",
    "    ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ ã€€ å½¡å½¡ ã€€ .ï¼¼\n",
    "    ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ ã€€ å½¡å½¡ã€€ã€€ã€€ .â— ãƒ½\n",
    "    ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ å½¡ã€€ã€€ ï¼ˆã€€ã€€ã€€ã€€ï¼¼\n",
    "    ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ ï¼¿â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ï¼¿ï¼¿ï¼¿ï¼¿â”€â”€../ã€€             /ãƒ½ ã€€ ãƒ½ã€€ã€€ ï¼ï¿£ï¿£ï¿£ï¿£ï¿£ï¿£ï¿£ï¿£\n",
    "    ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ï¼ã€€ã€€ã€€ ã€€ ã€€ /ã€€ ã€€ ã€€ ./ã€€ã€€ã€€ã€€ã€€ã€€ ã€€ ./ã€€ã€€ãƒ½ã€€oä¸¿ã€€ï¼œã€€ãƒ¯ãƒ¼ãƒ—ã«ä½¿ã£ã¦ãã ã•ã„\n",
    "    ã€€ ã€€ ã€€ ã€€ ã€€ ã€€ ã€€ å½¡å½¡å½¡ã€€ ã€€ ã€€ ã€€ ã€€ ./ã€€ã€€ã€€ã€€ã€€/ã€€ã€€ã€€ã€€ã€€ ã€€ ã€€  â”‚ ã€€ ã€€ ã€€ ã€€ ã€€ï¼¼ï¼¿ï¼¿ï¼¿ï¼¿ï¼¿ï¼¿ï¼¿ï¼¿\n",
    "    ã€€ã€€ ã€€ ã€€ ã€€ ã€€ å½¡å½¡å½¡å½¡ã€€ã€€ã€€ ã€€ ã€€ ã€€  |ï½ºï¾ï½°ï¾™ï¾„ï¾ï½¼ï½¯ï¾Œï¾Ÿ/ã€€ã€€ã€€ã€€ã€€ã€€ ã€€ ã€€ â”‚\n",
    "    ã€€ã€€ ã€€ ã€€ å½¡å½¡å½¡ã€€ã€€ã€€ /ã€€ã€€ã€€ã€€ã€€ã€€ã€€ ..ï¿£ï¿£ï¿£ï¿£ï¿£ã€€ .ãƒ½ã€ã€€ ã€€ ã€€ ãƒ½ã€€ï¾‰\n",
    "    ã€€ã€€ ã€€ å½¡å½¡ã€€ã€€ã€€ã€€ ã€€ ï¾‰ã€€ ã€€ ã€€ ï¼ï¿£ï¿£ï½€ ãƒ½ ï½¤ã€€ã€€ ã€€ ã€€ ã€€ ï½€ï½¤ã€€ã€€ ã€€ ï¾‰ã€€ï¼¼\n",
    "    ã€€ã€€ã€€ å½¡ã€€ã€€ã€€ã€€ã€€ã€€ ï¼ã€€ã€€ ï¼ã€€ï½€ï½¤ã€€ã€€ã€€ï¼ã€€ã€€ ï½€ã€€ãƒ¼ ï½¤ ï¼¿ï¼¿ãƒ½ã€€ã€€ ãƒ½ã€€ ã€€  ãƒ½ ã€\n",
    "    ã€€ã€€ã€€ã€€ã€€ ã€€ ã€€ ã€€ ï¼ ã€€ ï¼ã€€ã€€ã€€ /ã€€ã€€ /ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ ã€€ ã€€ ã€€ ï¼¼ã€€ ä¸¶ã€€- ï½¤ ï½€ï½¤\n",
    "    ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€/ã€€.ï¼ã€€ã€€ã€€ã€€ï¼ ã€€ ï¼ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ ï¼¼ã€€ï½€ï½¤ã€€ã€€ï¼¼ï¼¼\n",
    "    ã€€ã€€ã€€ã€€ ã€€ ã€€ .ï¼.ï¼ã€€ã€€ã€€ã€€ã€€ |ã€€ã€€/ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ ã€€  ï¼¼ ä¸¶ ã€€ ã€€ãƒ½ ãƒ½\n",
    "    ã€€ã€€ã€€ ã€€ ___ï¼.ï¼ã€€ ã€€ ã€€ ã€€ ã€€ |ã€€|ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ãƒ½ï½€ï½¤ ã€€ ã€€ï½€ï½¤ï½€ï½¤\n",
    "    ã€€ã€€ã€€ã€€/ |__ï¼ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ | |ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ï½€ï½¤ãƒ½ã€ã€€ã€€ ï½”ï¾†ã‚\n",
    "    ã€€ã€€ ã€€ ï¿£ã€€ã€€ ã€€ ã€€ ã€€ ã€€ ã€€ \"\"\"\"''\"\"'\"\" ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ ã€€ ã€€ ã€€ ãƒ½ ï½¤ãƒ½ã€€ã€€ã€€\n",
    "    ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ ã€€ ã€€ ã€€ ã€€ ã€€ ï½”ï¾†ã‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã«å…¥åŠ›ã—ã¦ï¼Œãƒ­ã‚°ã‚’è¦‹ã‚‹  \n",
    "conda activate env_pytorch  \n",
    "tensorboard --logdir runs  \n",
    "çµ‚ã‚ã‚‹ã¨ãâ¡ctrlã‚­ãƒ¼ã¨cã‚’åŒæ™‚ã«å…¥åŠ›  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
