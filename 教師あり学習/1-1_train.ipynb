{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries ğŸ“š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import easydict\n",
    "from glob import glob\n",
    "from natsort import natsorted\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch.utils.data as data\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import tensorboardX as tbx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random seedğŸŒ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#å†ç¾æ€§ã®ç¢ºä¿\n",
    "seed = 44\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    #Disabling the benchmark by CUDA convolution operation(GPUã‚’ä½¿ã†ã¨ãã®å†ç¾æ€§ã®æ‹…ä¿) (https://pytorch.org/docs/stable/notes/randomness.html)\n",
    "    torch.backends.cudnn.benchmark = False \n",
    "    torch.backends.cudnn.determinstic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments ğŸ“—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict({\n",
    "    \"batch_size\" : 32, # ãƒãƒƒãƒã‚µã‚¤ã‚º\n",
    "    \"learning_rate\" : 0.0001, # æå¤±ãŒä¸‹ãŒã‚Šãã‚‰ãªã„å ´åˆã¯ä¸‹ã’ã‚‹ã¨ã„ã„ã‹ã‚‚ï¼Ÿã€€adamã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ0.001\n",
    "    \"epochs\":1000, # å­¦ç¿’å›æ•°\n",
    "    \"early_stop_patience\" : 15, # æå¤±ãŒä¸‹ãŒã‚‰ãªã‹ã£ãŸã‚‰å­¦ç¿’ã‚’æ‰“ã¡åˆ‡ã‚‹å›æ•°\n",
    "\n",
    "    \"train_size\" : 40000,\n",
    "    \"valid_size\" : 10000,\n",
    "\n",
    "    \"model_save_path\" : \"check_point\", # ãƒ¢ãƒ‡ãƒ«ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª\n",
    "    \"model_file_name\" : \"model_data40000\", # ãƒ¢ãƒ‡ãƒ«å\n",
    "\n",
    "    \"gamma\" : 1, # ç‰¹ã«æ„å‘³ãªã—\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU ğŸ“º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_GPU():\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒã‚¤ã‚¹ã‚’å‡ºåŠ›ã™ã‚‹é–¢æ•°\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    device : object\n",
    "        GPUãŒä½¿ãˆã‚‹ãªã‚‰'cuda:0',ä½¿ãˆãªã„ãªã‚‰'cpu'ã‚’è¿”ã™\n",
    "    \"\"\"\n",
    "    print(\"Check GPU\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"You can use GPU({torch.cuda.get_device_name()})\")\n",
    "        d_type = \"cuda:0\"\n",
    "    else:\n",
    "        print(\"You use cpu\")\n",
    "        d_type = \"cpu\"\n",
    "    print(\"-----\")\n",
    "    device = torch.device(d_type)\n",
    "    return device "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre Procceing ğŸ›"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_normalization(train_df, num=500):\n",
    "    \"\"\"\n",
    "    ç”»åƒã®æ¨™æº–åŒ–ã‚’è¡Œã†ãŸã‚ã«ãƒ‡ãƒ¼ã‚¿ã®å¹³å‡ã¨æ¨™æº–åå·®ã‚’æ±‚ã‚ã‚‹é–¢æ•°\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img_path_list : list\n",
    "        globã§é›†ã‚ãŸç”»åƒpath\n",
    "    num : int\n",
    "        å¹³å‡ã¨æ¨™æº–åå·®ã‚’æ±‚ã‚ã‚‹ã®ã«ä½¿ã†ç”»åƒæšæ•° \n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    mean : tuple\n",
    "        ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç”»ç´ ã®å¹³å‡\n",
    "    std : tuple\n",
    "        ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç”»ç´ ã®æ¨™æº–åå·®    \n",
    "    \"\"\"\n",
    "\n",
    "    tensors = []\n",
    "    to_tensor = transforms.ToTensor()\n",
    "    sample_df = train_df.sample(num)\n",
    "    img_path_list = sample_df[\"fname\"]\n",
    "    for img in img_path_list:\n",
    "        img = Image.open(img)\n",
    "        img = to_tensor(img)\n",
    "        tensors.append(img)\n",
    "    tensors = torch.stack([img_t for img_t in tensors], dim=3) #dimã¯ã©ã“ã§ã‚‚ã‚ˆã„\n",
    "    mean = tensors.view(1,-1).mean(dim=1).item()\n",
    "    std = tensors.view(1,-1).std(dim=1).item()\n",
    "    \n",
    "    df = pd.DataFrame({\"mean\":[mean],\n",
    "                       \"std\":[std]})\n",
    "    df.to_csv(\"mean_std.csv\")\n",
    "    \n",
    "    return mean, std    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Set ğŸ§°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTransform:\n",
    "    \"\"\"\n",
    "    pytorchã§ã®ç”»åƒã®å‰å‡¦ç†ã‚¯ãƒ©ã‚¹\n",
    "    è¨“ç·´æ™‚ã ã‘ã€ãƒ‡ãƒ¼ã‚¿ã‚ªãƒ¼ã‚®ãƒ¥ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³(DA)ãŒã§ãã‚‹ã‚ˆã†ã«ã€train,validã§åˆ†ã‘ã¦æ›¸ã„ãŸ\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    mean : int\n",
    "        ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç”»ç´ å€¤ã®å¹³å‡å€¤\n",
    "    std : int\n",
    "        ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç”»ç´ å€¤ã®æ¨™æº–åå·®\n",
    "    \"\"\"   \n",
    "\n",
    "    def __init__(self, mean, std):\n",
    "        self.data_transform = {\n",
    "            'train':transforms.Compose([\n",
    "                #transforms.Resize(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std) \n",
    "            ]),\n",
    "            'valid':transforms.Compose([\n",
    "                #transforms.Resize(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std)\n",
    "            ])\n",
    "        }\n",
    "    \n",
    "    #å¼•æ•°ãªã—ã§å‘¼ã°ã‚ŒãŸã¨ãã®æŒ™å‹•ã‚’å®šç¾©\n",
    "    def __call__(self, img, phase='train'):\n",
    "        return self.data_transform[phase](img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image_Dataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    ç”»åƒã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¯ãƒ©ã‚¹ã€‚\n",
    "    Pytorch Dataset class ã‚’ç¶™æ‰¿\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        å­¦ç¿’ãƒ‡ãƒ¼ã‚¿       \n",
    "    transform : object\n",
    "        class ImageTransform()    \n",
    "    phase : str\n",
    "        'train' or 'valid'\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df, transform, phase='train'):\n",
    "        self.file_list = df[\"fname\"] \n",
    "        self.labels = df[[\"dx\",\"dy\"]].to_numpy()\n",
    "        self.l = df[\"l\"]\n",
    "        self.transform = transform  \n",
    "        self.phase = phase  \n",
    "\n",
    "    #ã“ã®ã‚¯ãƒ©ã‚¹ã®ç”»åƒæšæ•°ã‚’å®šç¾©ã€‚\n",
    "    def __len__(self):\n",
    "        return len(self.file_list) \n",
    "\n",
    "    #ã“ã®ã‚¯ãƒ©ã‚¹ã«è§’æ‹¬å¼§ã§ã‚¢ã‚¯ã‚»ã‚¹ã—ãŸæ™‚ã®æŒ™å‹•ã‚’å®šç¾©\n",
    "    def __getitem__(self, index):\n",
    "        # ç”»åƒã‚’ãƒ­ãƒ¼ãƒ‰\n",
    "        img_path = self.file_list[index]\n",
    "        img = Image.open(img_path)\n",
    "        #img = img.convert(\"L\") #ã‚°ãƒ¬ã‚¤ã‚¹ã‚±ãƒ¼ãƒ« \n",
    "        # å‰å‡¦ç†\n",
    "        img_transformed = self.transform(\n",
    "            img, self.phase)  # torch.Size([width, height]) \n",
    "\n",
    "        #label\n",
    "        label = self.labels[index]\n",
    "        #Tensorã«å¤‰æ›\n",
    "        label = torch.tensor(label, dtype=torch.float32) \n",
    "\n",
    "        # ãƒ¯ãƒ¼ã‚­ãƒ³ã‚°ãƒ‡ã‚£ã‚¹ã‚¿ãƒ³ã‚¹\n",
    "        l = self.l[index]           \n",
    "\n",
    "        return img_transformed, label, #l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NetWork ğŸ§ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # æ®‹å·®ãƒ–ãƒ­ãƒƒã‚¯\n",
    "\n",
    "\n",
    "class block(nn.Module):\n",
    "    def __init__(self, first_conv_in_channels, first_conv_out_channels, identity_conv=None, stride=1):\n",
    "        \"\"\"\n",
    "        æ®‹å·®ãƒ–ãƒ­ãƒƒã‚¯ã‚’ä½œæˆã™ã‚‹ã‚¯ãƒ©ã‚¹\n",
    "        Args:\n",
    "            first_conv_in_channels : 1ç•ªç›®ã®convå±¤ï¼ˆ1Ã—1ï¼‰ã®input channelæ•°\n",
    "            first_conv_out_channels : 1ç•ªç›®ã®convå±¤ï¼ˆ1Ã—1ï¼‰ã®output channelæ•°\n",
    "            identity_conv : channelæ•°èª¿æ•´ç”¨ã®convå±¤\n",
    "            stride : 3Ã—3convå±¤ã«ãŠã‘ã‚‹stideæ•°ã€‚sizeã‚’åŠåˆ†ã«ã—ãŸã„ã¨ãã¯2ã«è¨­å®š\n",
    "        \"\"\"        \n",
    "        super(block, self).__init__()\n",
    "\n",
    "        # 1ç•ªç›®ã®convå±¤ï¼ˆ1Ã—1ï¼‰\n",
    "        self.conv1 = nn.Conv2d(first_conv_in_channels, first_conv_out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(first_conv_out_channels)\n",
    "\n",
    "        # 2ç•ªç›®ã®convå±¤ï¼ˆ3Ã—3ï¼‰\n",
    "        # ãƒ‘ã‚¿ãƒ¼ãƒ³3ã®æ™‚ã¯sizeã‚’å¤‰æ›´ã§ãã‚‹ã‚ˆã†ã«strideã¯å¯å¤‰\n",
    "        self.conv2 = nn.Conv2d(first_conv_out_channels, first_conv_out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(first_conv_out_channels)\n",
    "\n",
    "        # 3ç•ªç›®ã®convå±¤ï¼ˆ1Ã—1ï¼‰\n",
    "        # output channelã¯input channelã®4å€ã«ãªã‚‹\n",
    "        self.conv3 = nn.Conv2d(first_conv_out_channels, first_conv_out_channels*4, kernel_size=1, stride=1, padding=0)\n",
    "        self.bn3 = nn.BatchNorm2d(first_conv_out_channels*4)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # identityã®channelæ•°ã®èª¿æ•´ãŒå¿…è¦ãªå ´åˆã¯convå±¤ï¼ˆ1Ã—1ï¼‰ã‚’ç”¨æ„ã€ä¸è¦ãªå ´åˆã¯None\n",
    "        self.identity_conv = identity_conv\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        identity = x.clone()  # å…¥åŠ›ã‚’ä¿æŒã™ã‚‹\n",
    "\n",
    "        x = self.conv1(x)  # 1Ã—1ã®ç•³ã¿è¾¼ã¿\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv2(x)  # 3Ã—3ã®ç•³ã¿è¾¼ã¿ï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³3ã®æ™‚ã¯strideãŒ2ã«ãªã‚‹ãŸã‚ã€ã“ã“ã§sizeãŒåŠåˆ†ã«ãªã‚‹ï¼‰\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.conv3(x)  # 1Ã—1ã®ç•³ã¿è¾¼ã¿\n",
    "        x = self.bn3(x)\n",
    "\n",
    "        # å¿…è¦ãªå ´åˆã¯convå±¤ï¼ˆ1Ã—1ï¼‰ã‚’é€šã—ã¦identityã®channelæ•°ã®èª¿æ•´ã—ã¦ã‹ã‚‰è¶³ã™\n",
    "        if self.identity_conv is not None:\n",
    "            identity = self.identity_conv(identity)\n",
    "        x += identity\n",
    "\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Resnet50\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self,block):\n",
    "        super(ResNet,self).__init__()\n",
    "\n",
    "        # conv1ã¯ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£é€šã‚Šã«ãƒ™ã‚¿æ‰“ã¡\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # conv2_xã¯ã‚µã‚¤ã‚ºã®å¤‰æ›´ã¯ä¸è¦ã®ãŸã‚ã€strideã¯1\n",
    "        self.conv2_x = self._make_layer(block, 3, res_block_in_channels=64, first_conv_out_channels=64, stride=1)\n",
    "\n",
    "        # conv3_xä»¥é™ã¯ã‚µã‚¤ã‚ºã®å¤‰æ›´ã‚’ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ãŸã‚ã€strideã¯2\n",
    "        self.conv3_x = self._make_layer(block, 4, res_block_in_channels=256,  first_conv_out_channels=128, stride=2)\n",
    "        self.conv4_x = self._make_layer(block, 6, res_block_in_channels=512,  first_conv_out_channels=256, stride=2)\n",
    "        self.conv5_x = self._make_layer(block, 3, res_block_in_channels=1024, first_conv_out_channels=512, stride=2)\n",
    "\n",
    "        #self.avgpool = nn.AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc1 = nn.Identity()\n",
    "        #self.fc2 = nn.Linear(224,2)\n",
    "\n",
    "        # Regression\n",
    "        self.regression = nn.Sequential(\n",
    "            nn.Linear(2048, 512), \n",
    "            nn.LeakyReLU(),       \n",
    "            nn.Linear(512, 128),  \n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, 32),        \n",
    "            nn.Linear(32, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        x = self.conv1(x)   # in:(3,224*224)ã€out:(64,112*112)\n",
    "        x = self.bn1(x)     # in:(64,112*112)ã€out:(64,112*112)\n",
    "        x = self.relu(x)    # in:(64,112*112)ã€out:(64,112*112)\n",
    "        x = self.maxpool(x) # in:(64,112*112)ã€out:(64,56*56)\n",
    "\n",
    "        x = self.conv2_x(x)  # in:(64,56*56)  ã€out:(256,56*56)\n",
    "        x = self.conv3_x(x)  # in:(256,56*56) ã€out:(512,28*28)\n",
    "        x = self.conv4_x(x)  # in:(512,28*28) ã€out:(1024,14*14)\n",
    "        x = self.conv5_x(x)  # in:(1024,14*14)ã€out:(2048,7*7)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        #x = self.fc2(x)\n",
    "\n",
    "        x = self.regression(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _make_layer(self, block, num_res_blocks, res_block_in_channels, first_conv_out_channels, stride):\n",
    "        layers = []\n",
    "\n",
    "        # 1ã¤ç›®ã®æ®‹å·®ãƒ–ãƒ­ãƒƒã‚¯ã§ã¯channelèª¿æ•´ã€åŠã³sizeèª¿æ•´ãŒç™ºç”Ÿã™ã‚‹\n",
    "        # identifyã‚’è¶³ã™å‰ã«1Ã—1ã®convå±¤ã‚’è¿½åŠ ã—ã€ã‚µã‚¤ã‚ºèª¿æ•´ãŒå¿…è¦ãªå ´åˆã¯strideã‚’2ã«è¨­å®š\n",
    "        identity_conv = nn.Conv2d(res_block_in_channels, first_conv_out_channels*4, kernel_size=1,stride=stride)\n",
    "        layers.append(block(res_block_in_channels, first_conv_out_channels, identity_conv, stride))\n",
    "\n",
    "        # 2ã¤ç›®ä»¥é™ã®input_channelæ•°ã¯1ã¤ç›®ã®output_channelã®4å€\n",
    "        in_channels = first_conv_out_channels*4\n",
    "\n",
    "        # channelèª¿æ•´ã€sizeèª¿æ•´ã¯ç™ºç”Ÿã—ãªã„ãŸã‚ã€identity_convã¯Noneã€strideã¯1\n",
    "        for i in range(num_res_blocks - 1):\n",
    "            layers.append(block(in_channels, first_conv_out_channels, identity_conv=None, stride=1))\n",
    "\n",
    "        return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function ğŸ›—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressison_loss_function(output, labels, args=args):\n",
    "    \"\"\"\n",
    "    å¹³å‡äºŒä¹—èª¤å·®ï¼ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆå­¦ç¿’ã¨åŒã˜å½¢ã«ã—ã¦ã‚‹ã ã‘\n",
    "    Attributes\n",
    "    ----------\n",
    "    output : torch.tensor\n",
    "        ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ã®å‡ºåŠ›\n",
    "    labels : torch.tensor\n",
    "        ç©ºæ°—æºã‚‰ããƒ©ãƒ™ãƒ«\n",
    "    Returns\n",
    "    ----------\n",
    "    loss : torch.tensor\n",
    "        å¹³å‡äºŒä¹—èª¤å·®(Loss_mse)\n",
    "    \"\"\"\n",
    "    criterion = nn.MSELoss()\n",
    "    loss = args.gamma * criterion(output, labels) / args.batch_size\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train ğŸ‹ï¸â€â™‚ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloaders_dict, device, opt, writer, earlystopping):\n",
    "\n",
    "    BREAK = False\n",
    "    print(f\"\\nStarting training\")\n",
    "    for epoch in range(0, args.epochs+1):\n",
    "\n",
    "        # epochã”ã¨ã®å­¦ç¿’ã¨æ¤œè¨¼ã®ãƒ«ãƒ¼ãƒ—\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ãƒ¢ãƒ¼ãƒ‰ã«\n",
    "            else:\n",
    "                model.eval()   # ãƒ¢ãƒ‡ãƒ«ã‚’æ¤œè¨¼ãƒ¢ãƒ¼ãƒ‰ã«\n",
    "\n",
    "            epoch_loss = 0.0  # epochã®æå¤±å’Œ                \n",
    "\n",
    "            #æœªè¨“ç·´æ™‚ã®æ€§èƒ½è©•ä¾¡\n",
    "            if (epoch == 0 and phase=='train'):\n",
    "                continue\n",
    "            \n",
    "            # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‹ã‚‰ãƒŸãƒ‹ãƒãƒƒãƒã‚’å–ã‚Šå‡ºã™ãƒ«ãƒ¼ãƒ—ã€‚args['batch_size']æšã”ã¨å–ã‚Šå‡ºã™\n",
    "            for inputs, labels in tqdm(dataloaders_dict[phase], leave=False, desc='Epoch {}/{} {}'.format(epoch, args.epochs, phase)): \n",
    "\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # optimizerã‚’åˆæœŸåŒ–\n",
    "                opt.zero_grad()\n",
    "\n",
    "                # é †ä¼æ¬ï¼ˆforwardï¼‰è¨ˆç®—\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = regressison_loss_function(outputs, labels)  # æå¤±ã‚’è¨ˆç®—ã€‚\n",
    "\n",
    "                    # è¨“ç·´æ™‚ã¯ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ‘ã‚²ãƒ¼ã‚·ãƒ§ãƒ³\n",
    "                    if phase == 'train':\n",
    "                        loss.backward() \n",
    "                        opt.step()\n",
    "\n",
    "                # æå¤±ã®è¨ˆç®—\n",
    "                if phase == 'train':\n",
    "                    epoch_loss += loss.item()*args.batch_size\n",
    "                elif phase == 'valid':\n",
    "                    epoch_loss += loss.item()*args.batch_size\n",
    "\n",
    "            #tensorboardã«å‡ºåŠ›\n",
    "            if phase == 'train':\n",
    "                writer.add_scalar('train_epoch_loss', epoch_loss, epoch) #(ã‚°ãƒ©ãƒ•å, yåº§æ¨™, xåº§æ¨™)                \n",
    "            elif phase == 'valid':\n",
    "                writer.add_scalar(f'valid_epoch_loss', epoch_loss, epoch)                  \n",
    "                #æ¯ã‚¨ãƒãƒƒã‚¯earlystoppingã®åˆ¤å®šã‚’ã•ã›ã‚‹\n",
    "                earlystopping(epoch_loss, model) #callãƒ¡ã‚½ãƒƒãƒ‰å‘¼ã³å‡ºã—\n",
    "                if earlystopping.early_stop: #ã‚¹ãƒˆãƒƒãƒ—ãƒ•ãƒ©ã‚°ãŒTrueã®å ´åˆã€breakã§forãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹\n",
    "                    print(\"Early Stopping!\")\n",
    "                    BREAK = True\n",
    "        if BREAK:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EarlyStopping â›”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    earlystoppingã‚¯ãƒ©ã‚¹\n",
    "    æå¤±ãŒä¸‹ãŒã£ãŸã“ã¨ã‚’åˆ¤æ–­ã—ã¦å­¦ç¿’ã‚’æ‰“ã¡åˆ‡ã‚‹\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    patience : int\n",
    "        ä½•å›æå¤±ãŒä¸‹ãŒã‚‰ãªã‹ã£ãŸã‚‰å­¦ç¿’ã‚’æ‰“ã¡åˆ‡ã‚‹ã‹\n",
    "    verbose : bool\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, patience, verbose):\n",
    "        \"\"\"å¼•æ•° : æœ€å°å€¤ã®éæ›´æ–°æ•°ã‚«ã‚¦ãƒ³ã‚¿ã€è¡¨ç¤ºè¨­å®š\"\"\"\n",
    "\n",
    "        self.patience = patience    #è¨­å®šã‚¹ãƒˆãƒƒãƒ—ã‚«ã‚¦ãƒ³ã‚¿\n",
    "        self.verbose = verbose      #è¡¨ç¤ºã®æœ‰ç„¡\n",
    "        self.counter = 0            #ç¾åœ¨ã®ã‚«ã‚¦ãƒ³ã‚¿å€¤\n",
    "        self.best_score = None      #ãƒ™ã‚¹ãƒˆã‚¹ã‚³ã‚¢\n",
    "        self.early_stop = False     #ã‚¹ãƒˆãƒƒãƒ—ãƒ•ãƒ©ã‚°\n",
    "        self.val_loss_min = np.Inf   #å‰å›ã®ãƒ™ã‚¹ãƒˆã‚¹ã‚³ã‚¢è¨˜æ†¶ç”¨\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        \"\"\"\n",
    "        ç‰¹æ®Š(call)ãƒ¡ã‚½ãƒƒãƒ‰\n",
    "        å®Ÿéš›ã«å­¦ç¿’ãƒ«ãƒ¼ãƒ—å†…ã§æœ€å°lossã‚’æ›´æ–°ã—ãŸã‹å¦ã‹ã‚’è¨ˆç®—ã•ã›ã‚‹éƒ¨åˆ†\n",
    "        \"\"\"\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:  #1Epochç›®ã®å‡¦ç†\n",
    "            self.best_score = score   #1Epochç›®ã¯ãã®ã¾ã¾ãƒ™ã‚¹ãƒˆã‚¹ã‚³ã‚¢ã¨ã—ã¦è¨˜éŒ²ã™ã‚‹\n",
    "            self.checkpoint(val_loss, model)  #è¨˜éŒ²å¾Œã«ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¦ã‚¹ã‚³ã‚¢è¡¨ç¤ºã™ã‚‹\n",
    "        elif score < self.best_score:  # ãƒ™ã‚¹ãƒˆã‚¹ã‚³ã‚¢ã‚’æ›´æ–°ã§ããªã‹ã£ãŸå ´åˆ\n",
    "            self.counter += 1   #ã‚¹ãƒˆãƒƒãƒ—ã‚«ã‚¦ãƒ³ã‚¿ã‚’+1\n",
    "            # if self.verbose:  #è¡¨ç¤ºã‚’æœ‰åŠ¹ã«ã—ãŸå ´åˆã¯çµŒéã‚’è¡¨ç¤º\n",
    "            #     print(f'EarlyStopping counter: {self.counter} out of {self.patience}')  #ç¾åœ¨ã®ã‚«ã‚¦ãƒ³ã‚¿ã‚’è¡¨ç¤ºã™ã‚‹ \n",
    "            if self.counter >= self.patience:  #è¨­å®šã‚«ã‚¦ãƒ³ãƒˆã‚’ä¸Šå›ã£ãŸã‚‰ã‚¹ãƒˆãƒƒãƒ—ãƒ•ãƒ©ã‚°ã‚’Trueã«å¤‰æ›´\n",
    "                self.early_stop = True\n",
    "        else:  #ãƒ™ã‚¹ãƒˆã‚¹ã‚³ã‚¢ã‚’æ›´æ–°ã—ãŸå ´åˆ\n",
    "            self.best_score = score  #ãƒ™ã‚¹ãƒˆã‚¹ã‚³ã‚¢ã‚’ä¸Šæ›¸ã\n",
    "            self.checkpoint(val_loss, model)  #ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¦ã‚¹ã‚³ã‚¢è¡¨ç¤º\n",
    "            self.counter = 0  #ã‚¹ãƒˆãƒƒãƒ—ã‚«ã‚¦ãƒ³ã‚¿ãƒªã‚»ãƒƒãƒˆ\n",
    "\n",
    "    def checkpoint(self, val_loss, model):\n",
    "        '''ãƒ™ã‚¹ãƒˆã‚¹ã‚³ã‚¢æ›´æ–°æ™‚ã«å®Ÿè¡Œã•ã‚Œã‚‹ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆé–¢æ•°'''\n",
    "        if self.verbose:  #è¡¨ç¤ºã‚’æœ‰åŠ¹ã«ã—ãŸå ´åˆã¯ã€å‰å›ã®ãƒ™ã‚¹ãƒˆã‚¹ã‚³ã‚¢ã‹ã‚‰ã©ã‚Œã ã‘æ›´æ–°ã—ãŸã‹ï¼Ÿã‚’è¡¨ç¤º\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        save_path = os.path.join(args.model_save_path, f'{args.model_file_name}.pth')\n",
    "        torch.save(model.state_dict(), save_path)  #ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®šã—ãŸpathã«ä¿å­˜\n",
    "        self.val_loss_min = val_loss  #ãã®æ™‚ã®lossã‚’è¨˜éŒ²ã™ã‚‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Main ğŸƒâ€â™€ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check GPU\n",
      "You can use GPU(NVIDIA GeForce RTX 3080)\n",
      "-----\n",
      "\n",
      "Starting training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 2790.492230).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (2790.492230 --> 2026.234539).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (2026.234539 --> 1526.072839).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (1526.072839 --> 769.001788).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (769.001788 --> 597.033066).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (597.033066 --> 448.278090).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (448.278090 --> 319.169289).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (319.169289 --> 302.218566).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (302.218566 --> 278.370721).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (278.370721 --> 275.519922).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (275.519922 --> 229.754846).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (229.754846 --> 204.603066).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (204.603066 --> 188.723561).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (188.723561 --> 172.844775).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (172.844775 --> 172.065712).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# æ™‚é–“è¨ˆæ¸¬é–‹å§‹\n",
    "time_start = time.time()\n",
    "\n",
    "#GPU\n",
    "device = check_GPU()\n",
    "\n",
    "os.makedirs(args.model_save_path, exist_ok=True) #é‡ã¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª\n",
    "# å­¦ç¿’æ¡ä»¶ã®ä¿å­˜\n",
    "csv_file = os.path.join(args.model_save_path,f\"{args.model_file_name}_info.csv\")\n",
    "with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    saver = csv.writer(file)\n",
    "    # ãƒ˜ãƒƒãƒ€ãƒ¼ï¼ˆè¾æ›¸ã®ã‚­ãƒ¼ï¼‰ã‚’æ›¸ãè¾¼ã‚€\n",
    "    saver.writerow(args.keys())\n",
    "    # ãƒ‡ãƒ¼ã‚¿ï¼ˆè¾æ›¸ã®å€¤ï¼‰ã‚’æ›¸ãè¾¼ã‚€\n",
    "    saver.writerow(args.values())\n",
    "\n",
    "train_df = pd.read_csv(\"train.csv\")[:args.train_size]\n",
    "valid_df = pd.read_csv(\"valid.csv\")[:args.valid_size]\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "dataset_dict = {\n",
    "        \"train\":train_df,\n",
    "        \"valid\":valid_df,\n",
    "        \"test\":test_df\n",
    "    }\n",
    "\n",
    "#ç”»åƒã®æ¨™æº–åŒ–\n",
    "mean, std = image_normalization(dataset_dict[\"train\"])\n",
    "\n",
    "transform = ImageTransform(mean, std)\n",
    "\n",
    "#logger\n",
    "writer = tbx.SummaryWriter()\n",
    "\n",
    "# Dataset\n",
    "train_dataset = Image_Dataset(df=dataset_dict[\"train\"], transform=transform, phase='train') \n",
    "valid_dataset = Image_Dataset(df=dataset_dict[\"valid\"], transform=transform, phase='valid')\n",
    "\n",
    "# make dataloader\n",
    "train_dataloader=torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    pin_memory=True,\n",
    "    shuffle=False,\n",
    "    )\n",
    "\n",
    "valid_dataloader=torch.utils.data.DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    pin_memory=True,\n",
    "    shuffle=False,\n",
    "    )\n",
    "\n",
    "# è¾æ›¸å‹å¤‰æ•°ã«ã¾ã¨ã‚ã‚‹\n",
    "dataloaders_dict = {\"train\": train_dataloader, \"valid\": valid_dataloader}          \n",
    "\n",
    "#Network\n",
    "model = ResNet(block)\n",
    "model.to(device)\n",
    "\n",
    "#optimizer\n",
    "opt = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "#early topping \n",
    "earlystopping = EarlyStopping(patience=args.early_stop_patience, verbose=True) \n",
    "\n",
    "#train\n",
    "train(model, dataloaders_dict, device, opt, writer, earlystopping)\n",
    "\n",
    "writer.close()\n",
    "\n",
    "#æ™‚é–“è¨ˆæ¸¬çµ‚äº†\n",
    "time_end = time.time()\n",
    "#çµŒéæ™‚é–“ï¼ˆç§’ï¼‰\n",
    "elapsed = time_end - time_start\n",
    "#ç§’â¡æ™‚é–“\n",
    "td = datetime.timedelta(seconds=elapsed)\n",
    "#çµŒéæ™‚é–“è¨˜æ†¶\n",
    "f = open('elapsed_time.txt','a')\n",
    "f.write(f\"{args.model_file_name}.pth : {str(td)} ,\")\n",
    "f.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€{ãƒ½ï¾âˆ§\n",
    "    ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ ã€€ å½¡å½¡ ã€€ .ï¼¼\n",
    "    ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ ã€€ å½¡å½¡ã€€ã€€ã€€ .â— ãƒ½\n",
    "    ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ å½¡ã€€ã€€ ï¼ˆã€€ã€€ã€€ã€€ï¼¼\n",
    "    ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ ï¼¿â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ï¼¿ï¼¿ï¼¿ï¼¿â”€â”€../ã€€             /ãƒ½ ã€€   ãƒ½ã€€ã€€ ï¼ï¿£ï¿£ï¿£ï¿£ï¿£ï¿£ï¿£ï¿£\n",
    "    ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ï¼ã€€ã€€ã€€ ã€€ ã€€ /ã€€ ã€€ ã€€ ./ã€€ã€€ã€€ã€€ã€€ã€€ ã€€ ./ã€€ã€€ãƒ½ã€€oä¸¿ã€€ï¼œã€€ãƒ¯ãƒ¼ãƒ—ã«ä½¿ã£ã¦ãã ã•ã„\n",
    "    ã€€ ã€€ ã€€ ã€€ ã€€ ã€€ ã€€ å½¡å½¡å½¡ã€€ ã€€ ã€€ ã€€ ã€€ ./ã€€ã€€ã€€ã€€ã€€/ã€€ã€€ã€€ã€€ã€€ ã€€ ã€€  â”‚ ã€€ ã€€ ã€€ ã€€ ã€€ ï¼¼ï¼¿ï¼¿ï¼¿ï¼¿ï¼¿ï¼¿ï¼¿ï¼¿\n",
    "    ã€€ã€€ ã€€ ã€€ ã€€ ã€€ å½¡å½¡å½¡å½¡ã€€ã€€ã€€ ã€€ ã€€ ã€€  |ï½ºï¾ï½°ï¾™ï¾„ï¾ï½¼ï½¯ï¾Œï¾Ÿ/ã€€ã€€ã€€ã€€ã€€ã€€ ã€€ ã€€ â”‚\n",
    "    ã€€ã€€ ã€€ ã€€ å½¡å½¡å½¡ã€€ã€€ã€€ /ã€€ã€€ã€€ã€€ã€€ã€€ã€€ ..ï¿£ï¿£ï¿£ï¿£ï¿£ã€€ .ãƒ½ã€ã€€ ã€€ ã€€ ãƒ½ã€€ï¾‰\n",
    "    ã€€ã€€ ã€€ å½¡å½¡ã€€ã€€ã€€ã€€ ã€€ ï¾‰ã€€ ã€€ ã€€ ï¼ï¿£ï¿£ï½€ ãƒ½ ï½¤ã€€ã€€ ã€€ ã€€ ã€€ ï½€ï½¤ã€€ã€€ ã€€ ï¾‰ã€€ï¼¼\n",
    "    ã€€ã€€ã€€ å½¡ã€€ã€€ã€€ã€€ã€€ã€€ ï¼ã€€ã€€ ï¼ã€€ï½€ï½¤ã€€ã€€ã€€ï¼ã€€ã€€ ï½€ã€€ãƒ¼ ï½¤ ï¼¿ï¼¿ãƒ½ã€€ã€€ ãƒ½ã€€ ã€€  ãƒ½ ã€\n",
    "    ã€€ã€€ã€€ã€€ã€€ ã€€ ã€€ ã€€ ï¼ ã€€ ï¼ã€€ã€€ã€€ /ã€€ã€€ /ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ ã€€ ã€€ ã€€ ï¼¼ã€€ ä¸¶ã€€- ï½¤ ï½€ï½¤\n",
    "    ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€/ã€€.ï¼ã€€ã€€ã€€ã€€ï¼ ã€€ ï¼ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ ï¼¼ã€€ï½€ï½¤ã€€ã€€ï¼¼ï¼¼\n",
    "    ã€€ã€€ã€€ã€€ ã€€ ã€€ .ï¼.ï¼ã€€ã€€ã€€ã€€ã€€ |ã€€ã€€/ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ ã€€  ï¼¼ ä¸¶ ã€€ ã€€ãƒ½ ãƒ½\n",
    "    ã€€ã€€ã€€ ã€€ ___ï¼.ï¼ã€€ ã€€ ã€€ ã€€ ã€€ |ã€€|ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ãƒ½ï½€ï½¤ ã€€ ã€€ï½€ï½¤ï½€ï½¤\n",
    "    ã€€ã€€ã€€ã€€/ |__ï¼ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ | |ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ï½€ï½¤ãƒ½ã€ã€€ã€€ ï½”ï¾†ã‚\n",
    "    ã€€ã€€ ã€€ ï¿£ã€€ã€€ ã€€ ã€€ ã€€ ã€€ ã€€ \"\"\"\"''\"\"'\"\" ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ ã€€ ã€€ ã€€ ãƒ½ ï½¤ãƒ½ã€€ã€€ã€€\n",
    "    ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ ã€€ ã€€ ã€€ ã€€ ã€€ ï½”ï¾†ã‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "47ea100d4d22683a1a742dc2bd2efe2b65c3502de462e7274ffa362f8c9b6784"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
